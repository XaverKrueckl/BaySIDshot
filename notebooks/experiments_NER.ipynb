{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVcOCrPFZyNq"
   },
   "source": [
    "# A Notebook for Basic Auxiliary Task Experiments on NER Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "elapsed": 16612,
     "status": "ok",
     "timestamp": 1718174927208,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     },
     "user_tz": -120
    },
    "id": "U-rXwrPTI-Mh",
    "outputId": "0af3c6c8-db83-4327-ef70-b24ab7eb1c35"
   },
   "outputs": [],
   "source": [
    "# mount google drive for access to unpublished data and saving results!\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# check if drive is present in root directory '/content'\n",
    "%ls -l\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 24383,
     "status": "ok",
     "timestamp": 1718174951582,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     },
     "user_tz": -120
    },
    "id": "dcr3N0fjTUxa",
    "outputId": "d03fc104-0335-4bc4-dd07-25d4e948fe1c"
   },
   "outputs": [],
   "source": [
    "# prepare/clear drive directory\n",
    "%rm -r /content/BaySIDshot # should fail in initial call\n",
    "%cd /content\n",
    "# get a fresh clone of BaySIDshot repo with it's submodules\n",
    "! git clone https://github.com/XaverKrueckl/BaySIDshot.git --recurse-submodules\n",
    "# cd into BaySIDshot repo:\n",
    "%cd /content/BaySIDshot/\n",
    "%ls -l\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# data for NER is split!\n",
    "# Wiki train-test-dev data accessible via\n",
    "# /content/BaySIDshot/BarNER/data/BarNER-final/bar-wiki-*.tsv\n",
    "\n",
    "# Twitter train-test-dev data may not be published for public - saved in private repo\n",
    "# /content/BaySIDshot/BarNER_Twitter/bar-tweet-*.tsv\n",
    "\n",
    "\n",
    "# finally move into machamp to start training and predictions\n",
    "%cd /content/BaySIDshot/machamp\n",
    "%pwd"
   ],
   "metadata": {
    "id": "uThIiRrjNwwJ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718174951583,
     "user_tz": -120,
     "elapsed": 14,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "b767b3bf-3048-4804-9fa7-f18abc0eadcf"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General Checks:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1718174951956,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     },
     "user_tz": -120
    },
    "id": "2RKo24KwfZZ3",
    "outputId": "a08a00a2-873a-40b6-af67-bb1ebea3c828"
   },
   "outputs": [],
   "source": [
    "# make sure to have a GPU backend selected\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 121523,
     "status": "ok",
     "timestamp": 1718175073476,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     },
     "user_tz": -120
    },
    "id": "Er1X18YZaSOd",
    "outputId": "6dfe9189-e3fa-48ed-e368-c52b9be159ba"
   },
   "outputs": [],
   "source": [
    "# install the required packages for machamp\n",
    "#! cat /content/BaySIDshot/machamp/README.md | grep \"requirements\"\n",
    "\n",
    "%cd /content/BaySIDshot/machamp\n",
    "! pip3 install --user -r /content/BaySIDshot/machamp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12995,
     "status": "ok",
     "timestamp": 1718175086467,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     },
     "user_tz": -120
    },
    "id": "QyQgq3dNmZuP",
    "outputId": "1b772b57-961e-4ed9-9735-8a9bdd6e5842"
   },
   "outputs": [],
   "source": [
    "# appends the directory /root/.local/bin to the existing PATH variable,\n",
    "# allowing executables located in that directory to be run from anywhere in the shell\n",
    "! export PATH=$PATH:/root/.local/bin\n",
    "\n",
    "# check if imports for machamp are there\n",
    "import tensorflow as tf\n",
    "# check the version\n",
    "print(tf.__version__)\n",
    "\n",
    "# check if basic system works\n",
    "! python3 /content/BaySIDshot/machamp/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare evaluation data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!bash /content/BaySIDshot/scripts/prepare_evaldata_dialects.sh\n",
    "#!bash /content/BaySIDshot/scripts/prepare_evaldata_baseline.sh\n",
    "!bash /content/BaySIDshot/scripts/prepare_evaldata_alllangs.sh\n",
    "\n",
    "# if issues occur, use manually created gold set:\n",
    "# /content/BaySIDshot/manual_data/alllangs_eval_data # or just dialects_eval_data, etc.\n",
    "\n",
    "%cd /content/BaySIDshot/machamp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Multitask NERxNLU setting:"
   ],
   "metadata": {
    "id": "zdmy1X_P3rOA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1718175145177,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     },
     "user_tz": -120
    },
    "id": "Ep39dLom-5Qu",
    "outputId": "f742fdb6-f68f-43ac-8269-91c9b43c5e9b"
   },
   "outputs": [],
   "source": [
    "# inspect configs to be used\n",
    "! cat /content/BaySIDshot/configs/ner_nlu_x.json\n",
    "\n",
    "# inspect params to be used\n",
    "! cat /content/BaySIDshot/configs/params_mdeberta.json"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# train experiment - make sure be in ../machamp or use direct path as below\n",
    "# set name of experiment / logs directory! --name mDeBERTa_exp2_nernlu_SEED\n",
    "# with respective random seed that should be used --seed 1234 e.g.\n",
    "\n",
    "\n",
    "! python3 /content/BaySIDshot/machamp/train.py --dataset_configs /content/BaySIDshot/configs/ner_nlu_x.json --parameters /content/BaySIDshot/configs/params_mdeberta.json --device 0 --name mDeBERTa_exp2_nernlu_1234 --seed 1234"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pj09k2d83he_",
    "outputId": "79e3a7fb-56fb-44c0-90c2-1b997ce2ab86",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718184513960,
     "user_tz": -120,
     "elapsed": 1173989,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# save logs dir with model and metrics to drive - change name of logs dir accordingly!\n",
    "\n",
    "! cp -R /content/BaySIDshot/machamp/logs/mDeBERTa_exp2_nernlu_1234* /content/drive/MyDrive/Masterarbeit"
   ],
   "metadata": {
    "id": "Gdt_gvDp3hAh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# SET and get path(s) to the final model\n",
    "# last line with text in model_path has model path to nlu model\n",
    "# (if nlu config was set as last one when sequential/intermediate training)\n",
    "! ls -d /content/drive/MyDrive/Masterarbeit/mDeBERTa_exp2_nernlu_1234*/*/model_* > model_path.txt\n",
    "\n",
    "import os\n",
    "\n",
    "with open('model_path.txt', 'r') as file:\n",
    "  lines = file.readlines()\n",
    "  # get the path to the final model (maily if sequential experiment)\n",
    "  model_line = [line.strip() for line in lines if line.strip()][-1]\n",
    "  model_line = model_line.strip()\n",
    "  print(\"Evaluating Model: \", model_line)\n",
    "  if '/' in model_line:\n",
    "    # just to show the parts of the path:\n",
    "    parts = model_line.split('/')\n",
    "    # get all the necessary path parts:\n",
    "    model = model_line.split('/')[7]\n",
    "    time = model_line.split('/')[6]\n",
    "    experiment_name = model_line.split('/')[5]\n",
    "    if experiment_name.split(\".\"):\n",
    "      experiment_name_cleaned = experiment_name.split(\".\")[0]\n",
    "    save_dir = model_line.split('/')[:5]\n",
    "    base_save_dir = '/'.join(save_dir)\n",
    "  else:\n",
    "    raise ValueError(\"No valid model path\")\n",
    "\n",
    "\n",
    "#eval_dir = \"/content/BaySIDshot/manual_data/alllangs_eval_data\"\n",
    "eval_dir = \"/content/BaySIDshot/alllangs_eval_data\"\n",
    "\n",
    "with open('script.sh', 'w') as file:\n",
    "    # create predictions folder in directory where model is saved:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # prepare prediction files order\n",
    "    file_list = []\n",
    "    for filename in os.listdir(eval_dir):\n",
    "        if filename.endswith('test.conll'): # do not use when also using valid files is desired\n",
    "          # path to goldfile:\n",
    "          goldfile = eval_dir + \"/\" + filename\n",
    "          # path to outfile\n",
    "          outfile = base_save_dir + \"/\" + experiment_name + \"/\" + time + \"/predictions_\" + experiment_name_cleaned + \"/\" + filename + \".out\"\n",
    "          # append goldfile outfile \"pairs\" to filelist for prediction command\n",
    "          file_list.append(str(goldfile))\n",
    "          file_list.append(str(outfile))\n",
    "    file_list_string = ' '.join(file_list)\n",
    "    # prediction call for all files in test dir\n",
    "    # also adds the specific dataset for which prediction should be done - necessary in multitask setting, else possible to also append\n",
    "    file.write(f\"! python3 predict.py {model_line} {file_list_string} --device 0 --dataset NLU\\n\")\n",
    "    # dir eval call to get all metrics and save output as json file:\n",
    "    file.write(f\"! python3 /content/BaySIDshot/scripts/dir_eval_out.py {eval_dir} {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # copy json output also to predictions dir:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/results\\n\")\n",
    "    file.write(f\"! cp /content/BaySIDshot/results/* {base_save_dir}/results\\n\")\n",
    "\n",
    "print(\"Prediction Script successfully generated!\")\n",
    "\n",
    "# runs predictions:\n",
    "! bash script.sh\n",
    "# and removes scripts:\n",
    "! rm script.sh\n",
    "! rm model_path.txt"
   ],
   "metadata": {
    "id": "eI_f3VCX3JLm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718184573141,
     "user_tz": -120,
     "elapsed": 56104,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "38c40803-03a1-482b-b5ea-4e6e7463cc40",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Sequential Intermediate NER_NLU setting:"
   ],
   "metadata": {
    "id": "_E1IsrfSJp5e"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# inspect configs to be used\n",
    "! cat /content/BaySIDshot/configs/ner_x.json\n",
    "! cat /content/BaySIDshot/configs/nlu_x.json\n",
    "\n",
    "# inspect params to be used\n",
    "! cat /content/BaySIDshot/configs/params_mdeberta.json"
   ],
   "metadata": {
    "id": "N3RvzUVp3NO7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718184685501,
     "user_tz": -120,
     "elapsed": 977,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "eb894424-aaa2-4580-a541-a4d46401e8d6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "g-IQ2moIuvx_",
    "outputId": "4cdf61e0-4c22-496f-c8d0-b888431ca666",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718194225695,
     "user_tz": -120,
     "elapsed": 1951704,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    }
   },
   "outputs": [],
   "source": [
    "# train experiment - make sure be in ../machamp or use direct path as below\n",
    "# set name of experiment / logs directory! --name mDeBERTa_exp2_ner_nlu_SEED\n",
    "# with respective random seed that should be used --seed 1234 e.g.\n",
    "\n",
    "\n",
    "! python3 /content/BaySIDshot/machamp/train.py --dataset_configs /content/BaySIDshot/configs/ner_x.json /content/BaySIDshot/configs/nlu_x.json --parameters /content/BaySIDshot/configs/params_mdeberta.json --sequential --device 0 --name mDeBERTa_exp2_ner_nlu_1234 --seed 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWg1Ea04J3qA"
   },
   "outputs": [],
   "source": [
    "# save logs dir with model and metrics to drive - change name of logs dir accordingly!\n",
    "\n",
    "! cp -R /content/BaySIDshot/machamp/logs/mDeBERTa_exp2_ner_nlu_1234* /content/drive/MyDrive/Masterarbeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfye40NkJ43_",
    "collapsed": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718194295398,
     "user_tz": -120,
     "elapsed": 58934,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "50870c1d-686e-40e3-88c3-9eb232e9b176"
   },
   "outputs": [],
   "source": [
    "# SET and get path(s) to the final model\n",
    "# last line with text in model_path has model path to nlu model\n",
    "# (if nlu config was set as last one when sequential/intermediate training)\n",
    "! ls -d /content/drive/MyDrive/Masterarbeit/mDeBERTa_exp2_ner_nlu_1234*/*/model_* > model_path.txt\n",
    "\n",
    "import os\n",
    "\n",
    "with open('model_path.txt', 'r') as file:\n",
    "  lines = file.readlines()\n",
    "  # get the path to the final model (maily if sequential experiment)\n",
    "  model_line = [line.strip() for line in lines if line.strip()][-1]\n",
    "  model_line = model_line.strip()\n",
    "  print(\"Evaluating Model: \", model_line)\n",
    "  if '/' in model_line:\n",
    "    # just to show the parts of the path:\n",
    "    parts = model_line.split('/')\n",
    "    # get all the necessary path parts:\n",
    "    model = model_line.split('/')[7]\n",
    "    time = model_line.split('/')[6]\n",
    "    experiment_name = model_line.split('/')[5]\n",
    "    if experiment_name.split(\".\"):\n",
    "      experiment_name_cleaned = experiment_name.split(\".\")[0]\n",
    "    save_dir = model_line.split('/')[:5]\n",
    "    base_save_dir = '/'.join(save_dir)\n",
    "  else:\n",
    "    raise ValueError(\"No valid model path\")\n",
    "\n",
    "\n",
    "#eval_dir = \"/content/BaySIDshot/manual_data/alllangs_eval_data\"\n",
    "eval_dir = \"/content/BaySIDshot/alllangs_eval_data\"\n",
    "\n",
    "with open('script.sh', 'w') as file:\n",
    "    # create predictions folder in directory where model is saved:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # prepare prediction files order\n",
    "    file_list = []\n",
    "    for filename in os.listdir(eval_dir):\n",
    "        if filename.endswith('test.conll'): # do not use when also using valid files is desired\n",
    "          # path to goldfile:\n",
    "          goldfile = eval_dir + \"/\" + filename\n",
    "          # path to outfile\n",
    "          outfile = base_save_dir + \"/\" + experiment_name + \"/\" + time + \"/predictions_\" + experiment_name_cleaned + \"/\" + filename + \".out\"\n",
    "          # append goldfile outfile \"pairs\" to filelist for prediction command\n",
    "          file_list.append(str(goldfile))\n",
    "          file_list.append(str(outfile))\n",
    "    file_list_string = ' '.join(file_list)\n",
    "    # prediction call for all files in test dir\n",
    "    # also adds the specific dataset for which prediction should be done - necessary in multitask setting, else possible to also append\n",
    "    file.write(f\"! python3 predict.py {model_line} {file_list_string} --device 0 --dataset NLU\\n\")\n",
    "    # dir eval call to get all metrics and save output as json file:\n",
    "    file.write(f\"! python3 /content/BaySIDshot/scripts/dir_eval_out.py {eval_dir} {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # copy json output also to predictions dir:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/results\\n\")\n",
    "    file.write(f\"! cp /content/BaySIDshot/results/* {base_save_dir}/results\\n\")\n",
    "\n",
    "print(\"Prediction Script successfully generated!\")\n",
    "\n",
    "# runs predictions:\n",
    "! bash script.sh\n",
    "# and removes scripts:\n",
    "! rm script.sh\n",
    "! rm model_path.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhcUQ7_A1y0t"
   },
   "source": [
    "### Reference for _MaChAmp_ and NER Data:\n",
    "\n",
    "```\n",
    "@inproceedings{van-der-goot-etal-2021-massive,\n",
    "    title = \"Massive Choice, Ample Tasks ({M}a{C}h{A}mp): A Toolkit for Multi-task Learning in {NLP}\",\n",
    "    author = {van der Goot, Rob  and\n",
    "      {\\\"U}st{\\\"u}n, Ahmet  and\n",
    "      Ramponi, Alan  and\n",
    "      Sharaf, Ibrahim  and\n",
    "      Plank, Barbara},\n",
    "    booktitle = \"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations\",\n",
    "    month = apr,\n",
    "    year = \"2021\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/2021.eacl-demos.22\",\n",
    "    doi = \"10.18653/v1/2021.eacl-demos.22\",\n",
    "    pages = \"176--197\",\n",
    "    abstract = \"Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of natural language processing tasks in a uniform toolkit, from text classification and sequence labeling to dependency parsing, masked language modeling, and text generation.\",\n",
    "}\n",
    "@inproceedings{peng-etal-2024-sebastian,\n",
    "    title = \"Sebastian, Basti, Wastl?! Recognizing Named Entities in {B}avarian Dialectal Data\",\n",
    "    author = \"Peng, Siyao  and\n",
    "      Sun, Zihang  and\n",
    "      Shan, Huangyan  and\n",
    "      Kolm, Marie  and\n",
    "      Blaschke, Verena  and\n",
    "      Artemova, Ekaterina  and\n",
    "      Plank, Barbara\",\n",
    "    editor = \"Calzolari, Nicoletta  and\n",
    "      Kan, Min-Yen  and\n",
    "      Hoste, Veronique  and\n",
    "      Lenci, Alessandro  and\n",
    "      Sakti, Sakriani  and\n",
    "      Xue, Nianwen\",\n",
    "    booktitle = \"Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)\",\n",
    "    month = may,\n",
    "    year = \"2024\",\n",
    "    address = \"Torino, Italia\",\n",
    "    publisher = \"ELRA and ICCL\",\n",
    "    url = \"https://aclanthology.org/2024.lrec-main.1262\",\n",
    "    pages = \"14478--14493\",\n",
    "    abstract = \"Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects. This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information. We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian. Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet. Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus. Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five NER and two Bavarian-German dialect identification tasks and achieve NER SOTA on bar-wiki. We substantiate the necessity of our low-resource BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance.\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn45VNCQJ8lX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": [],
   "collapsed_sections": [
    "zdmy1X_P3rOA",
    "XSKII9Ex3ajW"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
