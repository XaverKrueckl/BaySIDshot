{
  // set different transformer models here:
  // "transformer_model": "google-bert/bert-base-multilingual-cased",
  // "transformer_model": "FacebookAI/xlm-roberta-base",
  "transformer_model": "microsoft/mdeberta-v3-base",
  // "transformer_model": "deepset/gbert-base",  // used "google-bert/bert-base-german-cased" but this is older
  "transformer_model_dim": 768,
  "reset_transformer_model": false,
  "random_seed": 8446, // change here was not immanent - set a different random seed as command line argument
  "numpy_seed": 1337, // these two seeds necessary??
  "pytorch_seed": 133,
  "default_dec_dataset_embeds_dim": 12,

  "encoder": {
    "dropout": 0.3, // adapted to hyperparams according to xSID appendix
    "max_input_length": 128,
    "update_weights_encoder": true
  },

  "decoders": {

    "default_decoder": {
      "loss_weight": 1.0,
      "metric": "accuracy",
      "topn": 1,
      "layers_to_use": [-1]
    },
    "classification": {
      "metric": "accuracy" // set this here as default metric - was not present in default param file
    },
    "dependency": {
      "arc_representation_dim": 768,
      "tag_representation_dim": 256,
      "metric": "las"
    },
    "mlm": {
      "metric": "perplexity"
    },
    "multiclas": {
      "metric": "multi_acc",
      "threshold": 0.7
    },
    "multiseq": {
      "metric": "multi_acc",
      "threshold": 0.7
    },
    "regression": {
      "metric": "avg_dist"
    },
    "seq": {
      "metric": "span_f1" // set this here as default metric - was not present in default param file
    },
    "seq_bio": {
      "metric": "span_f1"
    },
    "string2string": {
      "metric": "accuracy" // sset this here as default metric - was not present in default param file
    },
    "tok": {
      "pre_split": true
    }
  },

  "batching": {
    "max_tokens": 1024,
    "shuffle": true,
    "batch_size": 32,
    "sort_by_size": true,
    "diverse": false,
    "sampling_smoothing": 1.0 // 1.0 == original size, 0.0==all equal
  },

  "training": {
        "keep_top_n": 1,
        "checkpointer": {
            "num_serialized_models_to_keep": 1
        },
        "use_amp": true, // could save some memory on gpu - from xSID baseline config
        "grad_norm": 1,
        "learning_rate_scheduler": {
            //"type": "slanted_triangular" // throws type error SlantedTriangular.__init__() got an unexpected keyword argument 'type'
            // also not set in default config - for newest machamp version!
            "cut_frac": 0.2,
            "decay_factor": 0.38,
            "discriminative_fine_tuning": true,
            "gradual_unfreezing": true
        },
        "num_epochs": 20,
        "optimizer": {
            //"type": "adamw", // also throws error AdamW.__init__() got an unexpected keyword argument 'type'
            // also not set in default config - for newest machamp version!
            "betas": [0.9, 0.99],
            "correct_bias": false,
            "lr": 0.0001, // not adapted to hyperparams according to xSID appendix - typo there?!
            
            "weight_decay": 0.01
        },
        //"patience": 5, // disabled, because slanted_triangular changes the lr dynamically
        //"validation_metric": "+.run/.sum" // do i need this? rather use default which is loss
    },
}

