
% Verena's paper on corpora for Germanic Low-Resource Languages and Dialects
@inproceedings{blaschke-etal-2023-survey,
    title = "A Survey of Corpora for {G}ermanic Low-Resource Languages and Dialects",
    author = "Blaschke, Verena  and Schuetze, Hinrich  and Plank, Barbara",
    editor = {Alum{\"a}e, Tanel  and Fishel, Mark},
    booktitle = "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may,
    year = "2023",
    address = "T{\'o}rshavn, Faroe Islands",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2023.nodalida-1.41",
    pages = "392--414",
    abstract = "Despite much progress in recent years, the vast majority of work in natural language processing (NLP) is on standard languages with many speakers. In this work, we instead focus on low-resource languages and in particular non-standardized low-resource languages. Even within branches of major language families, often considered well-researched, little is known about the extent and type of available resources and what the major NLP challenges are for these language varieties. The first step to address this situation is a systematic survey of available corpora (most importantly, annotated corpora, which are particularly valuable for NLP research). Focusing on Germanic low-resource language varieties, we provide such a survey in this paper. Except for geolocation (origin of speaker or document), we find that manually annotated linguistic resources are sparse and, if they exist, mostly cover morphosyntax. Despite this lack of resources, we observe that interest in this area is increasing: there is active development and a growing research community. To facilitate research, we make our overview of over 80 corpora publicly available.",
}


% Verenas survey paper on what dialect speakers want and need
@misc{blaschke2024dialectsurvey,
      title={What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects}, 
      author={Verena Blaschke and Christoph Purschke and Hinrich Schütze and Barbara Plank},
      year={2024},
      eprint={2402.11968},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11968}, 
}

% baseline paper from van der Goot
@inproceedings{van-der-goot-etal-2021-masked,
    title = "From Masked Language Modeling to Translation: Non-{E}nglish Auxiliary Tasks Improve Zero-shot Spoken Language Understanding",
    author = {van der Goot, Rob  and Sharaf, Ibrahim  and Imankulova, Aizhan  and {\"U}st{\"u}n, Ahmet  and Stepanovi{\'c}, Marija  and Ramponi, Alan  and Khairunnisa, Siti Oryza  and Komachi, Mamoru  and Plank, Barbara},
    editor = "Toutanova, Kristina  and Rumshisky, Anna  and Zettlemoyer, Luke  and Hakkani-Tur, Dilek  and Beltagy, Iz  and Bethard, Steven  and Cotterell, Ryan  and Chakraborty, Tanmoy  and Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.197",
    doi = "10.18653/v1/2021.naacl-main.197",
    pages = "2479--2497",
    abstract = "The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification.",
}


% when use version 0.4 of xSID  cite
@inproceedings{2023-findings-vardial,
  title = "Findings of the {V}ar{D}ial Evaluation Campaign 2023",
  author = {Aepli, No{\"e}mi and {\c{C}}{\"o}ltekin, {\c{C}}a{\u{g}}r{\i} and van der Goot, Rob and Jauhiainen, Tommi and Kazzaz, Mourhaf and Ljube{\v{s}}i{\'c}, Nikola and North, Kai and Plank, Barbara and Scherrer, Yves and Zampieri, Marcos},
  booktitle = "Proceedings of the Tenth Workshop on NLP for Similar Languages, Varieties and Dialects",
  month = may,
  year = "2023",
  address = "Dubrovnik, Croatia",
  publisher = "Association for Computational Linguistics",
}


% backbone of xSID baseline - MaChAmp paper/architecture
@inproceedings{van-der-goot-etal-2021-massive,
    title = "Massive Choice, Ample Tasks ({M}a{C}h{A}mp): A Toolkit for Multi-task Learning in {NLP}",
    author = {van der Goot, Rob  and {\"U}st{\"u}n, Ahmet  and Ramponi, Alan  and Sharaf, Ibrahim  and Plank, Barbara},
    editor = "Gkatzia, Dimitra  and Seddah, Djam{\'e}",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-demos.22",
    doi = "10.18653/v1/2021.eacl-demos.22",
    pages = "176--197",
    abstract = "Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of natural language processing tasks in a uniform toolkit, from text classification and sequence labeling to dependency parsing, masked language modeling, and text generation.",
}


% on Bavarian dialect:
@incollection{wiesinger1983deutschedialekte,
    author = {Peter Wiesinger},
    title = {Die Einteilung der deutschen Dialekte},
    booktitle = {Ergebnisse dialektologischer Beschreibungen: Areale Bereiche deutscher Dialekte im Überblick},
    editor = {Werner Besch and Ulrich Knoop and Wolfgang Putschke and Herbert E. Wiegand},
    publisher = {De Gruyter Mouton},
    address = {Berlin, Boston},
    pages = {807--960},
    doi = {10.1515/9783110203332-003},
    isbn = {9783110203332},
    year = {1983},
    lastchecked = {2024-06-26}
}


% general paper on Zero-Shot Slot and Intent Detection in Low-Resource Languages - also part of VarDial23
@inproceedings{kwon-etal-2023-sidlr,
    title = "{SIDLR}: Slot and Intent Detection Models for Low-Resource Language Varieties",
    author = "Kwon, Sang Yun  and Bhatia, Gagan  and Nagoudi, Elmoatez Billah  and Alcoba Inciarte, Alcides  and Abdul-mageed, Muhammad",
    editor = {Scherrer, Yves  and Jauhiainen, Tommi  and Ljube{\v{s}}i{\'c}, Nikola  and Nakov, Preslav  and Tiedemann, J{\"o}rg  and Zampieri, Marcos},
    booktitle = "Tenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2023)",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.vardial-1.24",
    doi = "10.18653/v1/2023.vardial-1.24",
    pages = "241--250",
    abstract = "Intent detection and slot filling are two critical tasks in spoken and natural language understandingfor task-oriented dialog systems. In this work, we describe our participation in slot and intent detection for low-resource language varieties (SID4LR) (Aepli et al., 2023). We investigate the slot and intent detection (SID) tasks using a wide range of models and settings. Given the recent success of multitask promptedfinetuning of the large language models, we also test the generalization capability of the recent encoder-decoder model mT0 (Muennighoff et al., 2022) on new tasks (i.e., SID) in languages they have never intentionally seen. We show that our best model outperforms the baseline by a large margin (up to +30 F1 points) in both SID tasks.",
}


% also a paper that was part of VarDial23 - Fine-tuning bert with character-level noise for zero-shot transfer to dialects and closely-related languages
@inproceedings{srivastava-chiang-2023-fine,
    title = "Fine-Tuning {BERT} with Character-Level Noise for Zero-Shot Transfer to Dialects and Closely-Related Languages",
    author = "Srivastava, Aarohi  and Chiang, David",
    editor = {Scherrer, Yves  and Jauhiainen, Tommi  and Ljube{\v{s}}i{\'c}, Nikola  and Nakov, Preslav  and Tiedemann, J{\"o}rg  and Zampieri, Marcos},
    booktitle = "Tenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2023)",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.vardial-1.16",
    doi = "10.18653/v1/2023.vardial-1.16",
    pages = "152--162",
    abstract = "In this work, we induce character-level noise in various forms when fine-tuning BERT to enable zero-shot cross-lingual transfer to unseen dialects and languages. We fine-tune BERT on three sentence-level classification tasks and evaluate our approach on an assortment of unseen dialects and languages. We find that character-level noise can be an extremely effective agent of cross-lingual transfer under certain conditions, while it is not as helpful in others. Specifically, we explore these differences in terms of the nature of the task and the relationships between source and target languages, finding that introduction of character-level noise during fine-tuning is particularly helpful when a task draws on surface level cues and the source-target cross-lingual pair has a relatively high lexical overlap with shorter (i.e., less meaningful) unseen tokens on average.",
}



% overall Survey on Natural language processing for similar languages, varieties, and dialects
@article{Zampieri_Nakov_Scherrer_2020, 
    title={Natural language processing for similar languages, varieties, and dialects: A survey}, 
    volume={26}, 
    DOI={10.1017/S1351324920000492}, 
    number={6}, 
    journal={Natural Language Engineering}, 
    author={Zampieri, Marcos and Nakov, Preslav and Scherrer, Yves}, 
    year={2020}, 
    pages={595–612}
}

% paper on end-to-end slot alignment and recognition for crosslingual nlu
@inproceedings{xu-etal-2020-end,
    title = "End-to-End Slot Alignment and Recognition for Cross-Lingual {NLU}",
    author = "Xu, Weijia  and Haider, Batool  and Mansour, Saab",
    editor = "Webber, Bonnie  and Cohn, Trevor  and He, Yulan  and Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.410",
    doi = "10.18653/v1/2020.emnlp-main.410",
    pages = "5052--5063",
    abstract = "Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes intent classification and slot labeling tasks. Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors. In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer. We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus. Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time. We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.",
}

% paper on cross-lingual transfer learning in the context of POS Tagging
@inproceedings{de-vries-etal-2022-make,
    title = "Make the Best of Cross-lingual Transfer: Evidence from {POS} Tagging with over 100 Languages",
    author = "de Vries, Wietse  and Wieling, Martijn  and Nissim, Malvina",
    editor = "Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.529",
    doi = "10.18653/v1/2022.acl-long.529",
    pages = "7676--7685",
    abstract = "Cross-lingual transfer learning with large multilingual pre-trained models can be an effective approach for low-resource languages with no labeled training data. Existing evaluations of zero-shot cross-lingual generalisability of large pre-trained models use datasets with English training data, and test data in a selection of target languages. We explore a more extensive transfer learning setup with 65 different source languages and 105 target languages for part-of-speech tagging. Through our analysis, we show that pre-training of both source and target language, as well as matching language families, writing systems, word order systems, and lexical-phonetic distance significantly impact cross-lingual performance. The findings described in this paper can be used as indicators of which factors are important for effective zero-shot cross-lingual transfer to zero- and low-resource languages.",
}

% paper on slot and intent detection done jointly for BERT
@misc{chen2019bertjointintentclassification,
      title={BERT for Joint Intent Classification and Slot Filling}, 
      author={Qian Chen and Zhu Zhuo and Wen Wang},
      year={2019},
      eprint={1902.10909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1902.10909}, 
}

% paper on imporoving slu with domain- and task-aware parameterization 
@misc{qin2021multidomainspokenlanguageunderstanding,
      title={Multi-Domain Spoken Language Understanding Using Domain- and Task-Aware Parameterization}, 
      author={Libo Qin and Minheng Ni and Yue Zhang and Wanxiang Che and Yangming Li and Ting Liu},
      year={2021},
      eprint={2004.14871},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.14871}, 
}


% Zero-Shot Algorithm for Intent and Slot Detection in Multilingual Task Oriented Languages
@inproceedings{jhan-etal-2022-c5l7,
    title = "$C5L7$: A Zero-Shot Algorithm for Intent and Slot Detection in Multilingual Task Oriented Languages",
    author = "Jhan, Jiun-hao  and Zhu, Qingxiaoyang  and Bengre, Nehal  and Kanungo, Tapas",
    editor = "FitzGerald, Jack  and Rottmann, Kay  and Hirschberg, Julia  and Bansal, Mohit  and Rumshisky, Anna  and Peris, Charith  and Hench, Christopher",
    booktitle = "Proceedings of the Massively Multilingual Natural Language Understanding Workshop (MMNLU-22)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.mmnlu-1.7",
    doi = "10.18653/v1/2022.mmnlu-1.7",
    pages = "62--68",
    abstract = "Voice assistants are becoming central to our lives. The convenience of using voice assistants to do simple tasks has created an industry for voice-enabled devices like TVs, thermostats, air conditioners, etc. It has also improved the quality of life of elders by making the world more accessible. Voice assistants engage in task-oriented dialogues using machine-learned language understanding models. However, training deep-learned models take a lot of training data, which is time-consuming and expensive. Furthermore, it is even more problematic if we want the voice assistant to understand hundreds of languages. In this paper, we present a zero-shot deep learning algorithm that uses only the English part of the Massive dataset and achieves a high level of accuracy across 51 languages. The algorithm uses a delexicalized translation model to generate multilingual data for data augmentation. The training data is further weighted to improve the accuracy of the worst-performing languages. We report on our experiments with code-switching, word order, multilingual ensemble methods, and other techniques and their impact on overall accuracy.",
}


% general overview on approaches in nlp for low resource scenarios
@inproceedings{hedderich-etal-2021-survey,
    title = "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios",
    author = {Hedderich, Michael A.  and Lange, Lukas  and Adel, Heike  and Str{\"o}tgen, Jannik  and Klakow, Dietrich},
    editor = "Toutanova, Kristina  and Rumshisky, Anna  and Zettlemoyer, Luke  and Hakkani-Tur, Dilek  and Beltagy, Iz  and Bethard, Steven  and Cotterell, Ryan  and Chakraborty, Tanmoy  and Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.201",
    doi = "10.18653/v1/2021.naacl-main.201",
    pages = "2545--2568",
    abstract = "Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research.",
}


% german varieties and task oriented dialogue systems
@inproceedings{artemova-etal-2024-exploring,
    title = "Exploring the Robustness of Task-oriented Dialogue Systems for Colloquial {G}erman Varieties",
    author = "Artemova, Ekaterina  and Blaschke, Verena  and Plank, Barbara",
    editor = "Graham, Yvette  and Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.28",
    pages = "445--468",
    abstract = "Mainstream cross-lingual task-oriented dialogue (ToD) systems leverage the transfer learning paradigm by training a joint model for intent recognition and slot-filling in English and applying it, zero-shot, to other languages.We address a gap in prior research, which often overlooked the transfer to lower-resource colloquial varieties due to limited test data.Inspired by prior work on English varieties, we craft and manually evaluate perturbation rules that transform German sentences into colloquial forms and use them to synthesize test sets in four ToD datasets.Our perturbation rules cover 18 distinct language phenomena, enabling us to explore the impact of each perturbation on slot and intent performance.Using these new datasets, we conduct an experimental evaluation across six different transformers.Here, we demonstrate that when applied to colloquial varieties, ToD systems maintain their intent recognition performance, losing 6{\%} (4.62 percentage points) in accuracy on average. However, they exhibit a significant drop in slot detection, with a decrease of 31{\%} (21 percentage points) in slot F$_1$ score.Our findings are further supported by a transfer experiment from Standard American English to synthetic Urban African American Vernacular English.",
}



% a survey on transfer learning with multilingual models on African low resource languages
@inproceedings{hedderich-etal-2020-transfer,
    title = "Transfer Learning and Distant Supervision for Multilingual Transformer Models: A Study on {A}frican Languages",
    author = "Hedderich, Michael A.  and Adelani, David  and Zhu, Dawei  and Alabi, Jesujoba  and Markus, Udia  and Klakow, Dietrich",
    editor = "Webber, Bonnie  and Cohn, Trevor  and He, Yulan  and Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.204",
    doi = "10.18653/v1/2020.emnlp-main.204",
    pages = "2580--2591",
    abstract = "Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages. However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios. In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and on both NER and topic classification. We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data. However, we also find settings where this does not hold. Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.",
}


% a survey on neural methods for SID for dialogue systems
@inproceedings{louvan-magnini-2020-recent,
    title = "Recent Neural Methods on Slot Filling and Intent Classification for Task-Oriented Dialogue Systems: A Survey",
    author = "Louvan, Samuel  and Magnini, Bernardo",
    editor = "Scott, Donia  and Bel, Nuria  and Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.42",
    doi = "10.18653/v1/2020.coling-main.42",
    pages = "480--496",
    abstract = "In recent years, fostered by deep learning technologies and by the high demand for conversational AI, various approaches have been proposed that address the capacity to elicit and understand user{'}s needs in task-oriented dialogue systems. We focus on two core tasks, slot filling (SF) and intent classification (IC), and survey how neural based models have rapidly evolved to address natural language understanding in dialogue systems. We introduce three neural architectures: independent models, which model SF and IC separately, joint models, which exploit the mutual benefit of the two tasks simultaneously, and transfer learning models, that scale the model to new domains. We discuss the current state of the research in SF and IC, and highlight challenges that still require attention.",
}

% SNIPS dataset ml paper SLU
@misc{coucke2018snips,
      title={Snips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces}, 
      author={Alice Coucke and Alaa Saade and Adrien Ball and Théodore Bluche and Alexandre Caulier and David Leroy and Clément Doumouro and Thibault Gisselbrecht and Francesco Caltagirone and Thibaut Lavril and Maël Primet and Joseph Dureau},
      year={2018},
      eprint={1805.10190},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% facebook dataset SLU
@inproceedings{schuster-etal-2019-cross-lingual,
    title = "Cross-lingual Transfer Learning for Multilingual Task Oriented Dialog",
    author = "Schuster, Sebastian  and Gupta, Sonal  and Shah, Rushin  and Lewis, Mike",
    editor = "Burstein, Jill  and Doran, Christy  and Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1380",
    doi = "10.18653/v1/N19-1380",
    pages = "3795--3805",
    abstract = "One of the first steps in the utterance interpretation pipeline of many task-oriented conversational AI systems is to identify user intents and the corresponding slots. Since data collection for machine learning models for this task is time-consuming, it is desirable to make use of existing data in a high-resource language to train models in low-resource languages. However, development of such models has largely been hindered by the lack of multilingual training data. In this paper, we present a new data set of 57k annotated utterances in English (43k), Spanish (8.6k) and Thai (5k) across the domains weather, alarm, and reminder. We use this data set to evaluate three different cross-lingual transfer methods: (1) translating the training data, (2) using cross-lingual pre-trained embeddings, and (3) a novel method of using a multilingual machine translation encoder as contextual word representations. We find that given several hundred training examples in the the target language, the latter two methods outperform translating the training data. Further, in very low-resource settings, multilingual contextual word representations give better results than using cross-lingual static embeddings. We also compare the cross-lingual methods to using monolingual resources in the form of contextual ELMo representations and find that given just small amounts of target language data, this method outperforms all cross-lingual methods, which highlights the need for more sophisticated cross-lingual methods.",
}

% Atis dataset base
@inproceedings{price-1990-evaluation,
    title = "Evaluation of Spoken Language Systems: the {ATIS} Domain",
    author = "Price, P. J.",
    booktitle = "Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",
    year = "1990",
    url = "https://aclanthology.org/H90-1020",
}

% Atis dataset additional
@inproceedings{hemphill-etal-1990-atis,
    title = "The {ATIS} Spoken Language Systems Pilot Corpus",
    author = "Hemphill, Charles T.  and Godfrey, John J.  and Doddington, George R.",
    booktitle = "Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",
    year = "1990",
    url = "https://aclanthology.org/H90-1021",
}

% Atis extension and paper with experiments:
@inproceedings{upandhyay_atis_zeroshot,
  author={Upadhyay, Shyam and Faruqui, Manaal and Tür, Gokhan and Dilek, Hakkani-Tür and Heck, Larry},
  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={(Almost) Zero-Shot Cross-Lingual Spoken Language Understanding}, 
  year={2018},
  volume={},
  number={},
  pages={6034-6038},
  keywords={Training;Training data;Data models;Task analysis;Google;Semantics;Reliability;Spoken Language Understanding;Cross-Lingual;Slot-Filling;Intent Classification},
  doi={10.1109/ICASSP.2018.8461905}
}


% a survey on SID datasets and some more analysis on this
@misc{larson2022survey,
      title={A Survey of Intent Classification and Slot-Filling Datasets for Task-Oriented Dialog}, 
      author={Stefan Larson and Kevin Leach},
      year={2022},
      eprint={2207.13211},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


% a SLU dataset for Italian also showed in xSID
@misc{bellomaria2019almawaveslu,
      title={Almawave-SLU: A new dataset for SLU in Italian}, 
      author={Valentina Bellomaria and Giuseppe Castellucci and Andrea Favalli and Raniero Romagnoli},
      year={2019},
      eprint={1907.07526},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

% newest and largest NLU dataset?
@inproceedings{fitzgerald-etal-2023-massive,
    title = "{MASSIVE}: A 1{M}-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages",
    author = "FitzGerald, Jack  and Hench, Christopher  and Peris, Charith  and Mackie, Scott  and Rottmann, Kay  and Sanchez, Ana  and Nash, Aaron  and Urbach, Liam  and Kakarala, Vishesh  and Singh, Richa  and Ranganath, Swetha  and Crist, Laurie  and Britan, Misha  and Leeuwis, Wouter  and Tur, Gokhan  and Natarajan, Prem",
    editor = "Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.235",
    doi = "10.18653/v1/2023.acl-long.235",
    pages = "4277--4302",
    abstract = "We present the MASSIVE dataset{--}Multilingual Amazon Slu resource package (SLURP) for Slot-filling, Intent classification, and Virtual assistant Evaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant utterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE was created by tasking professional translators to localize the English-only SLURP dataset into 50 typologically diverse languages from 29 genera. We also present modeling results on XLM-R and mT5, including exact match accuracy, intent classification accuracy, and slot-filling F1 score. We have released our dataset, modeling code, and models publicly.",
}

% SLURP used for MASSIVE
@inproceedings{bastianelli-2020-slurp,
    title = "{SLURP}: A Spoken Language Understanding Resource Package",
    author = "Bastianelli, Emanuele  and Vanzo, Andrea  and Swietojanski, Pawel  and Rieser, Verena",
    editor = "Webber, Bonnie  and Cohn, Trevor  and He, Yulan  and Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.588",
    doi = "10.18653/v1/2020.emnlp-main.588",
    pages = "7252--7262",
    abstract = "Spoken Language Understanding infers semantic meaning directly from audio data, and thus promises to reduce error propagation and misunderstandings in end-user applications. However, publicly available SLU resources are limited. In this paper, we release SLURP, a new SLU package containing the following: (1) A new challenging dataset in English spanning 18 domains, which is substantially bigger and linguistically more diverse than existing datasets; (2) Competitive baselines based on state-of-the-art NLU and ASR systems; (3) A new transparent metric for entity labelling which enables a detailed error analysis for identifying potential areas of improvement. SLURP is available at \url{https://github.com/pswietojanski/slurp}.",
}

% NLU++ dataset, presenting a richer NLU set
@inproceedings{casanueva-etal-2022-nlu,
    title = "{NLU}++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue",
    author = "Casanueva, Inigo  and Vuli{\'c}, Ivan and Spithourakis, Georgios  and Budzianowski, Pawe{\l}",
    editor = "Carpuat, Marine  and de Marneffe, Marie-Catherine  and Meza Ruiz, Ivan Vladimir",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.154",
    doi = "10.18653/v1/2022.findings-naacl.154",
    pages = "1998--2013",
    abstract = "We present NLU++, a novel dataset for natural language understanding (NLU) in task-oriented dialogue (ToD) systems, with the aim to provide a much more challenging evaluation environment for dialogue NLU models, up to date with the current application and industry requirements. NLU++ is divided into two domains (BANKING and HOTELS) and brings several crucial improvements over current commonly used NLU datasets. 1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences combined with finer-grained and thus more challenging slot sets. 2) The ontology is divided into domain-specific and generic (i.e., domain-universal) intents that overlap across domains, promoting cross-domain reusability of annotated examples. 3) The dataset design has been inspired by the problems observed in industrial ToD systems, and 4) it has been collected, filtered and carefully annotated by dialogue NLU experts, yielding high-quality annotated data. Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, and call for further research on ToD NLU.",
}

% multilingual NLU dataset with multiple intends and domains for dialogue
@inproceedings{moghe-etal-2023-multi3nlu,
    title = "{M}ulti3{NLU}++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue",
    author = "Moghe, Nikita  and Razumovskaia, Evgeniia  and Guillou, Liane  and Vuli{\'c}, Ivan  and Korhonen, Anna  and Birch, Alexandra",
    editor = "Rogers, Anna  and Boyd-Graber, Jordan  and Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.230",
    doi = "10.18653/v1/2023.findings-acl.230",
    pages = "3732--3755",
    abstract = "Task-oriented dialogue (ToD) systems have been widely deployed in many industries as they deliver more efficient customer support. These systems are typically constructed for a single domain or language and do not generalise well beyond this. To support work on Natural Language Understanding (NLU) in ToD across multiple languages and domains simultaneously, we constructed Multi3NLU++, a multilingual, multi-intent, multi-domain dataset. Multi3NLU++ extends the English-only NLU++ dataset to include manual translations into a range of high, medium, and low resource languages (Spanish, Marathi, Turkish and Amharic), in two domains (banking and hotels). Because of its multi-intent property, Multi3NLU++ represents complex and natural user goals, and therefore allows us to measure the realistic performance of ToD systems in a varied set of the world{'}s languages. We use Multi3NLU++ to benchmark state-of-the-art multilingual models for the NLU tasks of intent detection and slot labeling for ToD systems in the multilingual setting. The results demonstrate the challenging nature of the dataset, particularly in the low-resource language setting, offering ample room for future experimentation in multi-domain multilingual ToD setups.",
}



% multi task learning overview paper 
@misc{ruder2017overviewmultitasklearningdeep,
      title={An Overview of Multi-Task Learning in Deep Neural Networks}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1706.05098},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.05098}, 
}

% paper on influence of auxiliary tasks for multi-task learning of sequence tagging tasks
@inproceedings{schroder-biemann-2020-estimating,
    title = "Estimating the influence of auxiliary tasks for multi-task learning of sequence tagging tasks",
    author = {Schr{\"o}der, Fynn  and Biemann, Chris},
    editor = "Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.268",
    doi = "10.18653/v1/2020.acl-main.268",
    pages = "2971--2985",
    abstract = "Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks. However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach. We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups. Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel. Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work. We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance. We provide an efficient, open-source implementation.",
}


% how to enable BERT to better deal wit dialectal data
@inproceedings{srivastava-chiang-2023-bertwich,
    title = "{BERT}wich: Extending {BERT}{'}s Capabilities to Model Dialectal and Noisy Text",
    author = "Srivastava, Aarohi  and Chiang, David",
    editor = "Bouamor, Houda  and Pino, Juan  and Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.1037",
    doi = "10.18653/v1/2023.findings-emnlp.1037",
    pages = "15510--15521",
    abstract = "Real-world NLP applications often deal with nonstandard text (e.g., dialectal, informal, or misspelled text). However, language models like BERT deteriorate in the face of dialect variation or noise. How do we push BERT{'}s modeling capabilities to encompass nonstandard text? Fine-tuning helps, but it is designed for specializing a model to a task and does not seem to bring about the deeper, more pervasive changes needed to adapt a model to nonstandard language. In this paper, we introduce the novel idea of sandwiching BERT{'}s encoder stack between additional encoder layers trained to perform masked language modeling on noisy text. We find that our approach, paired with recent work on including character-level noise in fine-tuning data, can promote zero-shot transfer to dialectal text, as well as reduce the distance in the embedding space between words and their noisy counterparts.",
}


% using multilingual colexification for better crosslingual transfer learning
@inproceedings{liu-etal-2023-crosslingual-transfer,
    title = "Crosslingual Transfer Learning for Low-Resource Languages Based on Multilingual Colexification Graphs",
    author = "Liu, Yihong  and Ye, Haotian  and Weissweiler, Leonie  and Pei, Renhao  and Schuetze, Hinrich",
    editor = "Bouamor, Houda  and Pino, Juan  and Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.562",
    doi = "10.18653/v1/2023.findings-emnlp.562",
    pages = "8376--8401",
    abstract = "In comparative linguistics, colexification refers to the phenomenon of a lexical form conveying two or more distinct meanings. Existing work on colexification patterns relies on annotated word lists, limiting scalability and usefulness in NLP. In contrast, we identify colexification patterns of more than 2,000 concepts across 1,335 languages directly from an unannotated parallel corpus. We then propose simple and effective methods to build multilingual graphs from the colexification patterns: \textbf{ColexNet} and \textbf{ColexNet+}. ColexNet{'}s nodes are concepts and its edges are colexifications. In ColexNet+, concept nodes are additionally linked through intermediate nodes, each representing an ngram in one of 1,334 languages. We use ColexNet+ to train $\overrightarrow{\mbox{ColexNet+}}$, high-quality multilingual embeddings that are well-suited for transfer learning. In our experiments, we first show that ColexNet achieves high recall on CLICS, a dataset of crosslingual colexifications. We then evaluate $\overrightarrow{\mbox{ColexNet+}}$ on roundtrip translation, sentence retrieval and sentence classification and show that our embeddings surpass several transfer learning baselines. This demonstrates the benefits of using colexification as a source of information in multilingual NLP.",
}



% on slot labeling - useful for slot detection theory?
@inproceedings{razumovskaia-etal-2023-transfer,
    title = "Transfer-Free Data-Efficient Multilingual Slot Labeling",
    author = "Razumovskaia, Evgeniia  and Vuli{\'c}, Ivan  and Korhonen, Anna",
    editor = "Bouamor, Houda  and Pino, Juan  and Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.369",
    doi = "10.18653/v1/2023.emnlp-main.369",
    pages = "6041--6055",
    abstract = "Slot labeling (SL) is a core component of task-oriented dialogue (TOD) systems, where slots and corresponding values are usually language-, task- and domain-specific. Therefore, extending the system to any new language-domain-task configuration requires (re)running an expensive and resource-intensive data annotation process. To mitigate the inherent data scarcity issue, current research on multilingual ToD assumes that sufficient English-language annotated data are always available for particular tasks and domains, and thus operates in a standard cross-lingual transfer setup. In this work, we depart from this often unrealistic assumption. We examine challenging scenarios where such transfer-enabling English annotated data cannot be guaranteed, and focus on bootstrapping multilingual data-efficient slot labelers in transfer-free scenarios directly in the target languages without any English-ready data. We propose a two-stage slot labeling approach (termed TWOSL) which transforms standard multilingual sentence encoders into effective slot labelers. In Stage 1, relying on SL-adapted contrastive learning with only a handful of SL-annotated examples, we turn sentence encoders into task-specific span encoders. In Stage 2, we recast SL from a token classification into a simpler, less data-intensive span classification task. Our results on two standard multilingual TOD datasets and across diverse languages confirm the effectiveness and robustness of TWOSL. It is especially effective for the most challenging transfer-free few-shot setups, paving the way for quick and data-efficient bootstrapping of multilingual slot labelers for TOD.",
}

% paper that shows that even small amounts of auxiliary task data already improve results 
@inproceedings{muller-etal-2021-unseen,
    title = "When Being Unseen from m{BERT} is just the Beginning: Handling New Languages With Multilingual Language Models",
    author = "Muller, Benjamin  and Anastasopoulos, Antonios  and Sagot, Beno{\^\i}t  and Seddah, Djam{\'e}",
    editor = "Toutanova, Kristina  and Rumshisky, Anna  and Zettlemoyer, Luke  and Hakkani-Tur, Dilek  and Beltagy, Iz  and Bethard, Steven  and Cotterell, Ryan  and Chakraborty, Tanmoy  and Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.38",
    doi = "10.18653/v1/2021.naacl-main.38",
    pages = "448--462",
    abstract = "Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages.",
}


% a paper on data augmentation for better multilingual language understanding
@inproceedings{razumovskaia-etal-2022-data,
    title = "Data Augmentation and Learned Layer Aggregation for Improved Multilingual Language Understanding in Dialogue",
    author = "Razumovskaia, Evgeniia  and Vuli{\'c}, Ivan  and Korhonen, Anna",
    editor = "Muresan, Smaranda  and Nakov, Preslav  and Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.160",
    doi = "10.18653/v1/2022.findings-acl.160",
    pages = "2017--2033",
    abstract = "Scaling dialogue systems to a multitude of domains, tasks and languages relies on costly and time-consuming data annotation for different domain-task-language configurations. The annotation efforts might be substantially reduced by the methods that generalise well in zero- and few-shot scenarios, and also effectively leverage external unannotated data sources (e.g., Web-scale corpora). We propose two methods to this aim, offering improved dialogue natural language understanding (NLU) across multiple languages: 1) Multi-SentAugment, and 2) LayerAgg. Multi-SentAugment is a self-training method which augments available (typically few-shot) training data with similar (automatically labelled) in-domain sentences from large monolingual Web-scale corpora. LayerAgg learns to select and combine useful semantic information scattered across different layers of a Transformer model (e.g., mBERT); it is especially suited for zero-shot scenarios as semantically richer representations should strengthen the model{'}s cross-lingual capabilities. Applying the two methods with state-of-the-art NLU models obtains consistent improvements across two standard multilingual NLU datasets covering 16 diverse languages. The gains are observed in zero-shot, few-shot, and even in full-data scenarios. The results also suggest that the two methods achieve a synergistic effect: the best overall performance in few-shot setups is attained when the methods are used together.",
}


% a paper on crosslingual dataset creation via Outline-Based Generation
@article{majewska-etal-2023-cross,
    title = "Cross-Lingual Dialogue Dataset Creation via Outline-Based Generation",
    author = "Majewska, Olga  and Razumovskaia, Evgeniia  and Ponti, Edoardo M.  and Vuli{\'c}, Ivan  and Korhonen, Anna",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "11",
    year = "2023",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2023.tacl-1.9",
    doi = "10.1162/tacl_a_00539",
    pages = "139--156",
    abstract = "Multilingual task-oriented dialogue (ToD) facilitates access to services and information for many (communities of) speakers. Nevertheless, its potential is not fully realized, as current multilingual ToD datasets{---}both for modular and end-to-end modeling{---}suffer from severe limitations. 1) When created from scratch, they are usually small in scale and fail to cover many possible dialogue flows. 2) Translation-based ToD datasets might lack naturalness and cultural specificity in the target language. In this work, to tackle these limitations we propose a novel outline-based annotation process for multilingual ToD datasets, where domain-specific abstract schemata of dialogue are mapped into natural language outlines. These in turn guide the target language annotators in writing dialogues by providing instructions about each turn{'}s intents and slots. Through this process we annotate a new large-scale dataset for evaluation of multilingual and cross-lingual ToD systems. Our Cross-lingual Outline-based Dialogue dataset (cod) enables natural language understanding, dialogue state tracking, and end-to-end dialogue evaluation in 4 diverse languages: Arabic, Indonesian, Russian, and Kiswahili. Qualitative and quantitative analyses of cod versus an equivalent translation-based dataset demonstrate improvements in data quality, unlocked by the outline-based approach. Finally, we benchmark a series of state-of-the-art systems for cross-lingual ToD, setting reference scores for future work and demonstrating that cod prevents over-inflated performance, typically met with prior translation-based ToD datasets.",
}


% a general paper on processing multilingual Task-Oriented Dialogue
@inproceedings{razumovskaia-etal-2022-natural,
    title = "Natural Language Processing for Multilingual Task-Oriented Dialogue",
    author = "Razumovskaia, Evgeniia  and Glava{\v{s}}, Goran  and Majewska, Olga  and Ponti, Edoardo  and Vuli{\'c}, Ivan",
    editor = "Benotti, Luciana  and Okazaki, Naoaki  and Scherrer, Yves  and Zampieri, Marcos",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-tutorials.8",
    doi = "10.18653/v1/2022.acl-tutorials.8",
    pages = "44--50",
    abstract = "Recent advances in deep learning have also enabled fast progress in the research of task-oriented dialogue (ToD) systems. However, the majority of ToD systems are developed for English and merely a handful of other widely spoken languages, e.g., Chinese and German. This hugely limits the global reach and, consequently, transformative socioeconomic potential of such systems. In this tutorial, we will thus discuss and demonstrate the importance of (building) multilingual ToD systems, and then provide a systematic overview of current research gaps, challenges and initiatives related to multilingual ToD systems, with a particular focus on their connections to current research and challenges in multilingual and low-resource NLP. The tutorial will aim to provide answers or shed new light to the following questions: a) Why are multilingual dialogue systems so hard to build: what makes multilinguality for dialogue more challenging than for other NLP applications and tasks? b) What are the best existing methods and datasets for multilingual and cross-lingual (task-oriented) dialog systems? How are (multilingual) ToD systems usually evaluated? c) What are the promising future directions for multilingual ToD research: where can one draw inspiration from related NLP areas and tasks?",
}

% on the multilinguality of mBERT
@inproceedings{pires-etal-2019-multilingual,
    title = "How Multilingual is Multilingual {BERT}?",
    author = "Pires, Telmo  and Schlinger, Eva  and Garrette, Dan",
    editor = "Korhonen, Anna  and Traum, David  and M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1493",
    doi = "10.18653/v1/P19-1493",
    pages = "4996--5001",
    abstract = "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
}

% CoNLL Format paper
@inproceedings{tjong-kim-sang-de-meulder-2003-introduction,
    title = "Introduction to the {C}o{NLL}-2003 Shared Task: Language-Independent Named Entity Recognition",
    author = "Tjong Kim Sang, Erik F.  and
      De Meulder, Fien",
    booktitle = "Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003",
    year = "2003",
    url = "https://aclanthology.org/W03-0419",
    pages = "142--147",
}

% paper on english intermediate-task-training 
@inproceedings{phang-etal-2020-english,
    title = "{E}nglish Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too",
    author = "Phang, Jason  and Calixto, Iacer  and Htut, Phu Mon  and Pruksachatkun, Yada  and Liu, Haokun  and Vania, Clara  and Kann, Katharina  and Bowman, Samuel R.",
    editor = "Wong, Kam-Fai  and Knight, Kevin  and Wu, Hua",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-main.56",
    pages = "557--575",
    abstract = "Intermediate-task training{---}fine-tuning a pretrained model on an intermediate task before fine-tuning again on the target task{---}often improves model performance substantially on language understanding tasks in monolingual English settings. We investigate whether English intermediate-task training is still helpful on non-English target tasks. Using nine intermediate language-understanding tasks, we evaluate intermediate-task transfer in a zero-shot cross-lingual setting on the XTREME benchmark. We see large improvements from intermediate training on the BUCC and Tatoeba sentence retrieval tasks and moderate improvements on question-answering target tasks. MNLI, SQuAD and HellaSwag achieve the best overall results as intermediate tasks, while multi-task intermediate offers small additional improvements. Using our best intermediate-task models for each target task, we obtain a 5.4 point improvement over XLM-R Large on the XTREME benchmark, setting the state of the art as of June 2020. We also investigate continuing multilingual MLM during intermediate-task training and using machine-translated intermediate-task data, but neither consistently outperforms simply performing English intermediate-task training.",
}

% another paper on intermediate task training
@inproceedings{pruksachatkun-etal-2020-intermediate,
    title = "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?",
    author = "Pruksachatkun, Yada  and Phang, Jason  and Liu, Haokun  and Htut, Phu Mon  and Zhang, Xiaoyi  and Pang, Richard Yuanzhe  and Vania, Clara  and Kann, Katharina  and Bowman, Samuel R.",
    editor = "Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.467",
    doi = "10.18653/v1/2020.acl-main.467",
    pages = "5231--5247",
    abstract = "While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.",
}

% paper on the question whether pre-training should be done
@misc{dery2022pretraining,
      title={Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative}, 
      author={Lucio M. Dery and Paul Michel and Ameet Talwalkar and Graham Neubig},
      year={2022},
      eprint={2109.07437},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% why it would be better to use a non-english dev set
@inproceedings{keung-etal-2020-dont,
    title = "Don{'}t Use {E}nglish Dev: On the Zero-Shot Cross-Lingual Evaluation of Contextual Embeddings",
    author = "Keung, Phillip and Lu, Yichao and Salazar, Julian and Bhardwaj, Vikas",
    editor = "Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.40",
    doi = "10.18653/v1/2020.emnlp-main.40",
    pages = "549--554",
    abstract = "Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language. However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers. We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks. English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs. These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R). We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set. Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.",
}

% Verena's MaiBaam Paper for UD Data
@inproceedings{blaschke-etal-2024-maibaam-multi,
    title = "{M}ai{B}aam: A Multi-Dialectal {B}avarian {U}niversal {D}ependency Treebank",
    author = {Blaschke, Verena  and Kova{\v{c}}i{\'c}, Barbara  and Peng, Siyao  and Sch{\"u}tze, Hinrich  and Plank, Barbara},
    editor = "Calzolari, Nicoletta  and Kan, Min-Yen  and Hoste, Veronique  and Lenci, Alessandro  and Sakti, Sakriani  and Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.953",
    pages = "10921--10938",
    abstract = "Despite the success of the Universal Dependencies (UD) project exemplified by its impressive language breadth, there is still a lack in {`}within-language breadth{'}: most treebanks focus on standard languages. Even for German, the language with the most annotations in UD, so far no treebank exists for one of its language varieties spoken by over 10M people: Bavarian. To contribute to closing this gap, we present the first multi-dialect Bavarian treebank (MaiBaam) manually annotated with part-of-speech and syntactic dependency information in UD, covering multiple text genres (wiki, fiction, grammar examples, social, non-fiction). We highlight the morphosyntactic differences between the closely-related Bavarian and German and showcase the rich variability of speakers{'} orthographies. Our corpus includes 15k tokens, covering dialects from all Bavarian-speaking areas spanning three countries. We provide baseline parsing and POS tagging results, which are lower than results obtained on German and vary substantially between different graph-based parsers. To support further research on Bavarian syntax, we make our dataset, language-specific guidelines and code publicly available.",
}


% Logans paper for Bavarian NER data
@inproceedings{peng-etal-2024-sebastian-basti,
    title = "Sebastian, Basti, Wastl?! Recognizing Named Entities in {B}avarian Dialectal Data",
    author = "Peng, Siyao  and Sun, Zihang  and Shan, Huangyan  and Kolm, Marie  and Blaschke, Verena  and Artemova, Ekaterina  and Plank, Barbara",
    editor = "Calzolari, Nicoletta  and Kan, Min-Yen  and Hoste, Veronique  and Lenci, Alessandro  and Sakti, Sakriani  and Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1262",
    pages = "14478--14493",
    abstract = "Named Entity Recognition (NER) is a fundamental task to extract key information from texts, but annotated resources are scarce for dialects. This paper introduces the first dialectal NER dataset for German, BarNER, with 161K tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets (bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The Bavarian dialect differs from standard German in lexical distribution, syntactic construction, and entity information. We conduct in-domain, cross-domain, sequential, and joint experiments on two Bavarian and three German corpora and present the first comprehensive NER results on Bavarian. Incorporating knowledge from the larger German NER (sub-)datasets notably improves on bar-wiki and moderately on bar-tweet. Inversely, training first on Bavarian contributes slightly to the seminal German CoNLL 2006 corpus. Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task learning between five NER and two Bavarian-German dialect identification tasks and achieve NER SOTA on bar-wiki. We substantiate the necessity of our low-resource BarNER corpus and the importance of diversity in dialects, genres, and topics in enhancing model performance.",
}

% Miriams paper on second Bavarian Testset and also natural Bavarian data
@inproceedings{winkler-etal-2024-slot-intent,
    title = "Slot and Intent Detection Resources for {B}avarian and {L}ithuanian: Assessing Translations vs Natural Queries to Digital Assistants",
    author = "Winkler, Miriam  and Juozapaityte, Virginija  and van der Goot, Rob  and Plank, Barbara",
    editor = "Calzolari, Nicoletta  and Kan, Min-Yen  and Hoste, Veronique  and Lenci, Alessandro and Sakti, Sakriani  and Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1297",
    pages = "14898--14915",
    abstract = "Digital assistants perform well in high-resource languages like English, where tasks like slot and intent detection (SID) are well-supported. Many recent SID datasets start including multiple language varieties. However, it is unclear how realistic these translated datasets are. Therefore, we extend one such dataset, namely xSID-0.4, to include two underrepresented languages: Bavarian, a German dialect, and Lithuanian, a Baltic language. Both language variants have limited speaker populations and are often not included in multilingual projects. In addition to translations we provide {``}natural{''} queries to digital assistants generated by native speakers. We further include utterances from another dataset for Bavarian to build the richest SID dataset available today for a low-resource dialect without standard orthography. We then set out to evaluate models trained on English in a zero-shot scenario on our target language variants. Our evaluation reveals that translated data can produce overly optimistic scores. However, the error patterns in translated and natural datasets are highly similar. Cross-dataset experiments demonstrate that data collection methods influence performance, with scores lower than those achieved with single-dataset translations. This work contributes to enhancing SID datasets for underrepresented languages, yielding NaLiBaSID, a new evaluation dataset for Bavarian and Lithuanian.",
}

% Ekaterinas Paper from which the MLM precleared data is taken
@inproceedings{artemova-plank-2023-low,
    title = "Low-resource Bilingual Dialect Lexicon Induction with Large Language Models",
    author = "Artemova, Ekaterina  and Plank, Barbara",
    editor = {Alum{\"a}e, Tanel  and Fishel, Mark},
    booktitle = "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)",
    month = may,
    year = "2023",
    address = "T{\'o}rshavn, Faroe Islands",
    publisher = "University of Tartu Library",
    url = "https://aclanthology.org/2023.nodalida-1.39",
    pages = "371--385",
    abstract = "Bilingual word lexicons map words in one language to their synonyms in another language. Numerous papers have explored bilingual lexicon induction (BLI) in high-resource scenarios, framing a typical pipeline that consists of two steps: (i) unsupervised bitext mining and (ii) unsupervised word alignment. At the core of those steps are pre-trained large language models (LLMs).In this paper we present the analysis of the BLI pipeline for German and two of its dialects, Bavarian and Alemannic. This setup poses a number of unique challenges, attributed to the scarceness of resources, relatedness of the languages and lack of standardization in the orthography of dialects. We analyze the BLI outputs with respect to word frequency and the pairwise edit distance. Finally, we release an evaluation dataset consisting of manual annotations for 1K bilingual word pairs labeled according to their semantic similarity.",
}



% paper that shows for QA Task that sequential training is quite rewarding for BERT
@inproceedings{clark-etal-2019-boolq,
    title = "{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions",
    author = "Clark, Christopher  and Lee, Kenton  and Chang, Ming-Wei  and Kwiatkowski, Tom  and Collins, Michael  and Toutanova, Kristina",
    editor = "Burstein, Jill  and Doran, Christy  and Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1300",
    doi = "10.18653/v1/N19-1300",
    pages = "2924--2936",
    abstract = "In this paper we study yes/no questions that are naturally occurring {---} meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4{\%} accuracy compared to 90{\%} accuracy of human annotators (and 62{\%} majority-baseline), leaving a significant gap for future work.",
}



% Models:

% mBERT - source on huggingface is BERT Paper, see also https://github.com/google-research/bert/blob/master/multilingual.md
@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and Ming{-}Wei Chang and Kenton Lee and Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% xlm-r
@article{DBLP:journals/corr/abs-1911-02116,
  author    = {Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzm{\'{a}}n and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
  title     = {Unsupervised Cross-lingual Representation Learning at Scale},
  journal   = {CoRR},
  volume    = {abs/1911.02116},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02116},
  eprinttype = {arXiv},
  eprint    = {1911.02116},
  timestamp = {Mon, 11 Nov 2019 18:38:09 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02116.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
%this would be the xlm-r acl paper:
@inproceedings{conneau-etal-2020-unsupervised,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and Khandelwal, Kartikay  and Goyal, Naman  and Chaudhary, Vishrav  and Wenzek, Guillaume  and Guzm{\'a}n, Francisco  and Grave, Edouard  and Ott, Myle  and Zettlemoyer, Luke  and Stoyanov, Veselin",
    editor = "Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}


% mDeBERTa papers - both to be cited?!
@misc{he2021debertav3,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{he2021deberta,
title={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XPZIaotutsD}
}

% gbert:
@inproceedings{chan-etal-2020-germans,
    title = "{G}erman{'}s Next Language Model",
    author = {Chan, Branden  and Schweter, Stefan  and M{\"o}ller, Timo},
    editor = "Scott, Donia  and Bel, Nuria  and Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.598",
    doi = "10.18653/v1/2020.coling-main.598",
    pages = "6788--6796",
    abstract = "In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. All trained models will be made publicly available to the research community.",
}

% paper that shows the prompt sensitivity of large language models and that we do not know to much about this yet:
@inproceedings{leidinger-etal-2023-language,
    title = "The language of prompting: What linguistic properties make a prompt successful?",
    author = "Leidinger, Alina  and van Rooij, Robert  and Shutova, Ekaterina",
    editor = "Bouamor, Houda  and Pino, Juan  and Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.618",
    doi = "10.18653/v1/2023.findings-emnlp.618",
    pages = "9210--9232",
    abstract = "The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with the task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on prompts which reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performance cannot generally be explained by perplexity, word frequency, word sense ambiguity or prompt length. Based on our results, we put forward a proposal for a more robust and comprehensive evaluation standard for prompting research.",
}

% xlm-15
@article{lample2019cross,
  title={Cross-lingual language model pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}


% paper for Bavarian: Geolinguistic structures of dialect phonology in the german-speaking alpine region
@article{VergeinerBülow+2023,
url = {https://doi.org/10.1515/opli-2022-0252},
title = {Geolinguistic structures of dialect phonology in the German-speaking Alpine region: A dialectometric approach using crowdsourcing data},
author = {Philip C. Vergeiner and Lars Bülow},
pages = {20220252},
volume = {9},
number = {1},
journal = {Open Linguistics},
doi = {doi:10.1515/opli-2022-0252},
year = {2023},
lastchecked = {2024-06-25}
}


% paper on "advanced" SID - Dialogue State Tracking
@inproceedings{mrksic-etal-2017-neural,
    title = "Neural Belief Tracker: Data-Driven Dialogue State Tracking",
    author = "Mrk{\v{s}}i{\'c}, Nikola  and {\'O} S{\'e}aghdha, Diarmuid  and Wen, Tsung-Hsien  and Thomson, Blaise  and Young, Steve",
    editor = "Barzilay, Regina  and Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1163",
    doi = "10.18653/v1/P17-1163",
    pages = "1777--1788",
    abstract = "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user{'}s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users{'} language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.",
}

% Jurafsky and Martin overview book
@book{jurafsky_martin_processing, 
    author = {Jurafsky, Daniel and Martin, James H.}, 
    title = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition - Draft}, 
    year = {2024},
    edition = {3rd},
} 

% BERT paper
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and Chang, Ming-Wei  and Lee, Kenton  and Toutanova, Kristina",
    editor = "Burstein, Jill  and Doran, Christy  and Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

% attention paper
@inproceedings{vaswani_attention,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, \L{}ukasz and Polosukhin, Illia},
    title = {Attention is all you need},
    year = {2017},
    isbn = {9781510860964},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
    booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
    pages = {6000–6010},
    numpages = {11},
    location = {Long Beach, California, USA},
    series = {NIPS'17}
}


% paper on improving BERT based model by introducing a gate mechanism
@article{zahng_joint_learning_BERT,
  author={Zhang, Zhichang and Zhang, Zhenwen and Chen, Haoyuan and Zhang, Zhiman},
  journal={IEEE Access}, 
  title={A Joint Learning Framework With BERT for Spoken Language Understanding}, 
  year={2019},
  volume={7},
  number={},
  pages={168849-168858},
  keywords={Task analysis;Bit error rate;Semantics;Decoding;Logic gates;Mathematical model;Training;Spoken language understanding;intent classification and slot filling;joint learning;intent-augmented mechanism;pre-trained language model},
  doi={10.1109/ACCESS.2019.2954766}
}


@inproceedings{zhang-etal-2022-pre-trained,
    title = "Are Pre-trained Transformers Robust in Intent Classification? A Missing Ingredient in Evaluation of Out-of-Scope Intent Detection",
    author = "Zhang, Jianguo  and Hashimoto, Kazuma  and Wan, Yao  and Liu, Zhiwei  and Liu, Ye  and Xiong, Caiming  and Yu, Philip",
    editor = "Liu, Bing  and Papangelis, Alexandros  and Ultes, Stefan  and Rastogi, Abhinav  and Chen, Yun-Nung  and Spithourakis, Georgios  and Nouri, Elnaz  and Shi, Weiyan",
    booktitle = "Proceedings of the 4th Workshop on NLP for Conversational AI",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.nlp4convai-1.2",
    doi = "10.18653/v1/2022.nlp4convai-1.2",
    pages = "12--20",
    abstract = "Pre-trained Transformer-based models were reported to be robust in intent classification. In this work, we first point out the importance of in-domain out-of-scope detection in few-shot intent recognition tasks and then illustrate the vulnerability of pre-trained Transformer-based models against samples that are in-domain but out-of-scope (ID-OOS). We construct two new datasets, and empirically show that pre-trained models do not perform well on both ID-OOS examples and general out-of-scope examples, especially on fine-grained few-shot intent detection tasks.",
}

% work on atis set and slu for dialogue systems
@inproceedings{tur_atis_2010,
  author={Tur, Gokhan and Hakkani-Tür, Dilek and Heck, Larry},
  booktitle={2010 IEEE Spoken Language Technology Workshop}, 
  title={What is left to be understood in ATIS?}, 
  year={2010},
  volume={},
  number={},
  pages={19-24},
  keywords={Training;Error analysis;Speech recognition;Training data;Semantics;Feature extraction;Cities and towns;spoken language understanding;ATIS;discriminative training},
  doi={10.1109/SLT.2010.5700816}
}

% paper on SLU data and lanuage understanding in general
@misc{bunk2020understanding,
      title={DIET: Lightweight Language Understanding for Dialogue Systems}, 
      author={Tanja Bunk and Daksh Varshneya and Vladimir Vlasov and Alan Nichol},
      year={2020},
      eprint={2004.09936},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.09936}, 
}

% paper on cross-lingual transferability 
@inproceedings{artetxe-etal-2020-cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and Ruder, Sebastian  and Yogatama, Dani",
    editor = "Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.421",
    doi = "10.18653/v1/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

% another paper on crosslingual transferability
@inproceedings{lewis-etal-2020-mlqa,
    title = "{MLQA}: Evaluating Cross-lingual Extractive Question Answering",
    author = "Lewis, Patrick  and Oguz, Barlas  and Rinott, Ruty  and Riedel, Sebastian  and Schwenk, Holger",
    editor = "Jurafsky, Dan  and Chai, Joyce  and Schluter, Natalie  and Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.653",
    doi = "10.18653/v1/2020.acl-main.653",
    pages = "7315--7330",
    abstract = "Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making building QA systems that work well in other languages challenging. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. MLQA has over 12K instances in English and 5K in each other language, with each instance parallel between 4 languages on average. We evaluate state-of-the-art cross-lingual models and machine-translation-based baselines on MLQA. In all cases, transfer results are shown to be significantly behind training-language performance.",
}

% paper on the effctiveness of multi-task learning
@inproceedings{alonso2017-multitask,
    title = "When is multitask learning effective? Semantic sequence prediction under varying data conditions",
    author = "Mart{\'\i}nez Alonso, H{\'e}ctor  and Plank, Barbara",
    editor = "Lapata, Mirella  and Blunsom, Phil  and Koller, Alexander",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-1005",
    pages = "44--53",
    abstract = "Multitask learning has been applied successfully to a range of tasks, mostly morphosyntactic. However, little is known on \textit{when} MTL works and whether there are data characteristics that help to determine the success of MTL. In this paper we evaluate a range of semantic sequence labeling tasks in a MTL setup. We examine different auxiliary task configurations, amongst which a novel setup, and correlate their impact to data-dependent conditions. Our results show that MTL is not always effective, because significant improvements are obtained only for 1 out of 5 tasks. When successful, auxiliary tasks with compact and more uniform label distributions are preferable.",
}

% paper on zero-shot cross-lingual transfer with mBERT
@misc{qin2020cosdamlmultilingual,
      title={CoSDA-ML: Multi-Lingual Code-Switching Data Augmentation for Zero-Shot Cross-Lingual NLP}, 
      author={Libo Qin and Minheng Ni and Yue Zhang and Wanxiang Che},
      year={2020},
      eprint={2006.06402},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.06402}, 
}

% paper on zero-shot cross-lingual transfer for dialogue systems
@misc{liu2019attention,
      title={Attention-Informed Mixed-Language Training for Zero-shot Cross-lingual Task-oriented Dialogue Systems}, 
      author={Zihan Liu and Genta Indra Winata and Zhaojiang Lin and Peng Xu and Pascale Fung},
      year={2019},
      eprint={1911.09273},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.09273}, 
}

% paper on zero-shot cross-lingual transfer for dialogue systems with Transferable Latent Variables
@inproceedings{liu-etal-2019-zero,
    title = "Zero-shot Cross-lingual Dialogue Systems with Transferable Latent Variables",
    author = "Liu, Zihan  and Shin, Jamin  and Xu, Yan  and Winata, Genta Indra  and Xu, Peng  and Madotto, Andrea  and Fung, Pascale",
    editor = "Inui, Kentaro  and Jiang, Jing  and Ng, Vincent  and Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1129",
    doi = "10.18653/v1/D19-1129",
    pages = "1297--1303",
    abstract = "Despite the surging demands for multilingual task-oriented dialog systems (e.g., Alexa, Google Home), there has been less research done in multilingual or cross-lingual scenarios. Hence, we propose a zero-shot adaptation of task-oriented dialogue system to low-resource languages. To tackle this challenge, we first use a set of very few parallel word pairs to refine the aligned cross-lingual word-level representations. We then employ a latent variable model to cope with the variance of similar sentences across different languages, which is induced by imperfect cross-lingual alignments and inherent differences in languages. Finally, the experimental results show that even though we utilize much less external resources, our model achieves better adaptation performance for natural language understanding task (i.e., the intent detection and slot filling) compared to the current state-of-the-art model in the zero-shot scenario.",
}


% paper showing that smaller sized transformer models are better for modeling low-resource languages
@misc{vanbiljon2020depthlowresource,
      title={On Optimal Transformer Depth for Low-Resource Language Translation}, 
      author={Elan van Biljon and Arnu Pretorius and Julia Kreutzer},
      year={2020},
      eprint={2004.04418},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2004.04418}, 
}

% paper showing that small models can learn low-resource languages quite well
@inproceedings{schick-schutze-2021-just,
    title = "It{'}s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
    author = {Schick, Timo  and Sch{\"u}tze, Hinrich},
    editor = "Toutanova, Kristina  and Rumshisky, Anna  and Zettlemoyer, Luke  and Hakkani-Tur, Dilek  and Beltagy, Iz  and Bethard, Steven  and Cotterell, Ryan  and Chakraborty, Tanmoy  and Zhou, Yichao",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.185",
    doi = "10.18653/v1/2021.naacl-main.185",
    pages = "2339--2352",
    abstract = "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much {``}greener{''} in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.",
}

% shared embedding spaces improve transfer capabilities
@article{Ruder_2019,
   title={A Survey of Cross-lingual Word Embedding Models},
   volume={65},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.1.11640},
   DOI={10.1613/jair.1.11640},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Ruder, Sebastian and Vulić, Ivan and Søgaard, Anders},
   year={2019},
   month=aug, pages={569–631} 
}

% paper that shows ho zero-shot language transfer with multilingual models is less effective for resource-lean settings and distant lan- guages
@inproceedings{lauscher-etal-2020-zero,
    title = "From Zero to Hero: {O}n the Limitations of Zero-Shot Language Transfer with Multilingual {T}ransformers",
    author = "Lauscher, Anne  and Ravishankar, Vinit  and Vuli{\'c}, Ivan  and Glava{\v{s}}, Goran",
    editor = "Webber, Bonnie  and Cohn, Trevor  and He, Yulan  and Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.363",
    doi = "10.18653/v1/2020.emnlp-main.363",
    pages = "4483--4499",
    abstract = "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance. Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages. In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining. Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
}

% paper on dialectal intent detection
@inproceedings{abboud-oz-2024-equitable,
    title = "Towards Equitable Natural Language Understanding Systems for Dialectal Cohorts: Debiasing Training Data",
    author = "Abboud, Khadige  and Oz, Gokmen",
    editor = "Calzolari, Nicoletta  and Kan, Min-Yen  and Hoste, Veronique  and Lenci, Alessandro  and Sakti, Sakriani  and Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1433",
    pages = "16487--16499",
    abstract = "Despite being widely spoken, dialectal variants of languages are frequently considered low in resources due to lack of writing standards and orthographic inconsistencies. As a result, training natural language understanding (NLU) systems relies primarily on standard language resources leading to biased and inequitable NLU technology that underserves dialectal speakers. In this paper, we propose to address this problem through a framework composed of a dialect identification model that is used to obtain targeted training data augmentation for under-represented dialects, in an effort to debias NLU model for dialectal cohorts in NLU systems. We conduct experiments on two dialect rich non-English languages: Arabic and German, using large-scale commercial NLU datasets as well as open-source datasets. Results show that such framework can provide insights on dialect disparity in real-world NLU systems and targeted data argumentation can help narrow the model{'}s performance gap between standard language speakers and dialect speakers.",
}

% paper on German dialects syntax structure
@article{weiß_dialect_structure,
 ISSN = {13834924, 15728552},
 URL = {http://www.jstor.org/stable/43549731},
 abstract = {In this paper, I present new data from several German dialects concerning the order of pronominal subjects and objects. The data are taken from three sources (Wenker survey, Bavarian Linguistic Atlas, Syntax of Hessian Dialects) and cover a time span of nearly 150 years, thus providing a very robust empirical basis. Although the canonical order in all German varieties is subject before object and this serialization holds for pronominal as well as for non-pronominal arguments, a certain proportiom of reverse orders for pronouns is attested in all three data sources. That is, there are speakers of various dialectal varieties who prefer object before subject as the unmarked order of pronouns (e.g., haben dir sie geholfen lit. 'have you they helped'). I will try to explain the emergence of the reverse order adopting the null subject cycle (NSC) proposed by Fuß and Wratil (2013) as a point of departure and modifying it into a more general pronoun cycle (PC): The PC implies the development of reduced pronouns out of full ones, which then replace clitic or null pronouns (depending on the person and number combination). The reverse order comprises the steps in the cycle where a clitic or a null subject pronoun is replaced with a reduced subject pronoun out of which eventually a clitic pronoun evolves. The reduced subject pronoun follows the object clitic as the clitic subject does as well for a short period of time before it is placed again before the object clitic. In the concluding section I discuss the observable mismatch between morphological paradigms and the syntactic level, in that syntax requires a tripartite distinction for which morphology mostly provides only two distinct forms.},
 author = {Helmut Weiß},
 journal = {The Journal of Comparative Germanic Linguistics},
 number = {1},
 pages = {65--92},
 publisher = {Springer},
 title = {When the subject follows the object. On a curiosity in the syntax of personal pronouns in some German dialects},
 urldate = {2024-07-17},
 volume = {18},
 year = {2015}
}

% paper on article usage in upper german
@article{vergeiner_articleS_dialect,
   author = "Vergeiner, Philip C. and Niehaus, Konstantin",
   title = "Article use in Upper German– a ‘radical’ stage of grammaticalization?: New findings from Austria", 
   journal= "Linguistic Variation",
   year = "2024",
   volume = "24",
   number = "1",
   pages = "37-76",
   doi = "https://doi.org/10.1075/lv.21014.ver",
   url = "https://www.jbe-platform.com/content/journals/10.1075/lv.21014.ver",
   publisher = "John Benjamins",
   issn = "2211-6834",
   type = "Journal Article",
   keywords = "determiners",
   keywords = "grammaticalization",
   keywords = "Bavarian",
   keywords = "Alemannic",
   keywords = "variation and change",
   abstract = "Despite an increasing interest in German dialect syntax, the study of article use in Upper German (Alemannic and Bavarian) remains a desideratum. This is true in particular for Austrian varieties. The present study focusses on article variation and change in Austrian Upper German and discusses the status of article grammaticalization. To that effect, ‘radical’ cases of article use in Upper German are analysed, i.e. cases considered incorrect in standard German: the use of indefinite articles before mass nouns, of definite articles before proper nouns, and of indefinite articles in the plural. These phenomena are investigated by means of a comprehensive dialect survey (3,599 dialect translations by 163 dialect speakers from 40 research locations). The analysis examines inner-linguistic factors (lexis, semantics, syntax) as well as extra-linguistic factors (dialect areas, age group). The findings reveal a surprisingly high variability and a relatively advanced stage of grammaticalization in some areas, especially Central Bavarian dialects.",
  }


