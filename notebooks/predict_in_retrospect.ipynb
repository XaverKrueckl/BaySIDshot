{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVcOCrPFZyNq"
   },
   "source": [
    "# A Notebook for retrospectively predicting on fine-tuned MaChAmp models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcr3N0fjTUxa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718089393986,
     "user_tz": -120,
     "elapsed": 25615,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "1c04ac2f-41b0-493d-aa5e-6a5731436aee"
   },
   "outputs": [],
   "source": [
    "# mount google drive for access to unpublished data and saving results!\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# check if drive is present in root directory '/content'\n",
    "%ls -l\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare/clear drive directory\n",
    "%rm -r /content/BaySIDshot # should fail in initial call\n",
    "%cd /content\n",
    "# get a fresh clone of BaySIDshot repo with it's submodules\n",
    "! git clone https://github.com/XaverKrueckl/BaySIDshot.git --recurse-submodules\n",
    "# cd into BaySIDshot repo:\n",
    "%cd /content/BaySIDshot/\n",
    "%ls -l\n",
    "%pwd"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6dub2nXXopwe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718089436267,
     "user_tz": -120,
     "elapsed": 37940,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "4788986c-f3ef-4c1e-c6b1-ef5a404b6e36"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General Checks:"
   ],
   "metadata": {
    "id": "FHppnU-NQkd9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# make sure to have a GPU backend selected\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U-rXwrPTI-Mh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718089439797,
     "user_tz": -120,
     "elapsed": 803,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "e3670c37-78e4-4a41-cb36-daff64baf92a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RKo24KwfZZ3",
    "outputId": "37f6b2e1-fc6b-4a62-9271-d05580dee635",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718089582982,
     "user_tz": -120,
     "elapsed": 140164,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install the required packages for machamp\n",
    "#! cat /content/BaySIDshot/machamp/README.md | grep \"requirements\"\n",
    "\n",
    "%cd /content/BaySIDshot/machamp\n",
    "! pip3 install --user -r /content/BaySIDshot/machamp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kf7GD5BaBvI",
    "outputId": "d9fa0d8c-60d3-44e8-fe6e-2d3b558130f8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718089601540,
     "user_tz": -120,
     "elapsed": 18565,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# appends the directory /root/.local/bin to the existing PATH variable,\n",
    "# allowing executables located in that directory to be run from anywhere in the shell\n",
    "! export PATH=$PATH:/root/.local/bin\n",
    "\n",
    "# check if imports for machamp are there\n",
    "import tensorflow as tf\n",
    "# check the version\n",
    "print(tf.__version__)\n",
    "\n",
    "# check if basic system works\n",
    "! python3 /content/BaySIDshot/machamp/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicting and Evaluating Retrospectively:"
   ],
   "metadata": {
    "id": "JCIeGoyhO3pC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# preparing prediction data:\n",
    "\n",
    "!bash /content/BaySIDshot/scripts/prepare_evaldata_dialects.sh\n",
    "!bash /content/BaySIDshot/scripts/prepare_evaldata_baseline.sh\n",
    "!bash /content/BaySIDshot/scripts/prepare_evaldata_alllangs.sh\n",
    "\n",
    "%cd /content/BaySIDshot/machamp\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VJW4x4btO2Uq",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1718089605152,
     "user_tz": -120,
     "elapsed": 3616,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "e89262db-76cf-4b44-e60a-dca8efd59073"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# get path to the final model\n",
    "# last line with text in model_path has model path to nlu model\n",
    "# (if nlu config was set as last one when sequential/intermediate training)\n",
    "# example with a path used in this master thesis:\n",
    "! ls -d /content/drive/MyDrive/Masterarbeit/Baseline_mDeBERTa_1234*/*/model_* > model_path.txt\n",
    "\n",
    "import os\n",
    "\n",
    "with open('model_path.txt', 'r') as file:\n",
    "  lines = file.readlines()\n",
    "  # get the path to the final model (maily if sequential experiment)\n",
    "  model_line = [line.strip() for line in lines if line.strip()][-1]\n",
    "  model_line = model_line.strip()\n",
    "  print(\"Evaluating Model: \", model_line)\n",
    "  if '/' in model_line:\n",
    "    # just to show the parts of the path:\n",
    "    parts = model_line.split('/')\n",
    "    # get all the necessary path parts:\n",
    "    model = model_line.split('/')[7]\n",
    "    time = model_line.split('/')[6]\n",
    "    experiment_name = model_line.split('/')[5]\n",
    "    if experiment_name.split(\".\"):\n",
    "      experiment_name_cleaned = experiment_name.split(\".\")[0]\n",
    "    save_dir = model_line.split('/')[:5]\n",
    "    base_save_dir = '/'.join(save_dir)\n",
    "  else:\n",
    "    raise ValueError(\"No valid model path\")\n",
    "\n",
    "\n",
    "#eval_dir = \"/content/BaySIDshot/manual_data/dialects_gold_data\"\n",
    "eval_dir = \"/content/BaySIDshot/alllangs_eval_data\"\n",
    "# other evaluation sets can be used here\n",
    "\n",
    "\n",
    "with open('script.sh', 'w') as file:\n",
    "    # create predictions folder in directory where model is saved:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # prepare prediction files order\n",
    "    file_list = []\n",
    "    for filename in os.listdir(eval_dir):\n",
    "        if filename.endswith('test.conll'): # do not use when also using valid files is desired\n",
    "          # path to goldfile:\n",
    "          goldfile = eval_dir + \"/\" + filename\n",
    "          # path to outfile\n",
    "          outfile = base_save_dir + \"/\" + experiment_name + \"/\" + time + \"/predictions_\" + experiment_name_cleaned + \"/\" + filename + \".out\"\n",
    "          # append goldfile outfile \"pairs\" to filelist for prediction command\n",
    "          file_list.append(str(goldfile))\n",
    "          file_list.append(str(outfile))\n",
    "    file_list_string = ' '.join(file_list)\n",
    "    # prediction call for all files in test dir\n",
    "    # also adds the specific dataset for which prediction should be done - necessary in multitask setting, else possible to also append\n",
    "    file.write(f\"! python3 predict.py {model_line} {file_list_string} --device 0 --dataset NLU\\n\")\n",
    "    # dir eval call to get all metrics and save output as json file:\n",
    "    file.write(f\"! python3 /content/BaySIDshot/scripts/dir_eval_out.py {eval_dir} {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # copy json output also to predictions dir:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/results\\n\")\n",
    "    file.write(f\"! cp /content/BaySIDshot/results/* {base_save_dir}/results\\n\")\n",
    "\n",
    "print(\"Prediction Script successfully generated!\")\n",
    "\n",
    "# runs predictions:\n",
    "! bash script.sh\n",
    "# and removes scripts:\n",
    "! rm script.sh\n",
    "! rm model_path.txt"
   ],
   "metadata": {
    "id": "tsz3XFnYk2R4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhcUQ7_A1y0t"
   },
   "source": [
    "### Reference for _MaChAmp_:\n",
    "\n",
    "```\n",
    "@inproceedings{van-der-goot-etal-2021-massive,\n",
    "    title = \"Massive Choice, Ample Tasks ({M}a{C}h{A}mp): A Toolkit for Multi-task Learning in {NLP}\",\n",
    "    author = {van der Goot, Rob  and\n",
    "      {\\\"U}st{\\\"u}n, Ahmet  and\n",
    "      Ramponi, Alan  and\n",
    "      Sharaf, Ibrahim  and\n",
    "      Plank, Barbara},\n",
    "    booktitle = \"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations\",\n",
    "    month = apr,\n",
    "    year = \"2021\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/2021.eacl-demos.22\",\n",
    "    doi = \"10.18653/v1/2021.eacl-demos.22\",\n",
    "    pages = \"176--197\",\n",
    "    abstract = \"Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of natural language processing tasks in a uniform toolkit, from text classification and sequence labeling to dependency parsing, masked language modeling, and text generation.\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": [
    "BBTQSAwGsDRB"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
