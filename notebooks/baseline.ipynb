{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVcOCrPFZyNq"
   },
   "source": [
    "# Recreating the xSID Baseline for zero-shot transfer learning on SID using MaChAmp in Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# mount google drive for access to unpublished data and saving results!\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# check if drive is present in root directory '/content'\n",
    "%ls -l\n",
    "%pwd"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "6dub2nXXopwe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716020770668,
     "user_tz": -120,
     "elapsed": 821,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "73d4a2ec-4e9a-461d-a6c2-ed12f8b728aa",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare/clear drive directory\n",
    "%rm -r /content/BaySIDshot # should fail in initial call\n",
    "%cd /content\n",
    "# get a fresh clone of BaySIDshot repo with it's submodules\n",
    "! git clone https://github.com/XaverKrueckl/BaySIDshot.git --recurse-submodules\n",
    "# cd into BaySIDshot repo:\n",
    "%cd /content/BaySIDshot/\n",
    "%ls -l\n",
    "%pwd"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "U-rXwrPTI-Mh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716020829134,
     "user_tz": -120,
     "elapsed": 18508,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "ba2dea7a-3e8d-48fe-dff1-79c3ea702d7a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## General Checks:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2RKo24KwfZZ3",
    "outputId": "91eab021-baf7-4716-cea9-d38251670d45",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716020859756,
     "user_tz": -120,
     "elapsed": 534,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    }
   },
   "outputs": [],
   "source": [
    "# make sure to have GPU backend, e.g. T4 (may also use cpu then change device later)\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Er1X18YZaSOd",
    "outputId": "ccc7b8d3-d5d2-4401-9e04-996d1a8c3e0f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716021000984,
     "user_tz": -120,
     "elapsed": 120814,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# install the required packages for machamp\n",
    "#! cat /content/BaySIDshot/machamp/README.md | grep \"requirements\"\n",
    "\n",
    "%cd /content/BaySIDshot/machamp\n",
    "! pip3 install --user -r /content/BaySIDshot/machamp/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyQgq3dNmZuP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716021014196,
     "user_tz": -120,
     "elapsed": 13216,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "26c96881-14d4-4dd1-d026-4797986a4e03"
   },
   "outputs": [],
   "source": [
    "# appends the directory /root/.local/bin to the existing PATH variable,\n",
    "# allowing executables located in that directory to be run from anywhere in the shell\n",
    "! export PATH=$PATH:/root/.local/bin\n",
    "\n",
    "# check if imports for machamp are there\n",
    "import tensorflow as tf\n",
    "# check the version\n",
    "print(tf.__version__)\n",
    "\n",
    "# check if basic system works\n",
    "! python3 train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare evaluation data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#!bash /content/BaySIDshot/scripts/prepare_evaldata_dialects.sh\n",
    "#!bash /content/BaySIDshot/scripts/prepare_evaldata_baseline.sh\n",
    "!bash /content/BaySIDshot/scripts/prepare_evaldata_alllangs.sh\n",
    "\n",
    "# if issues occur, use manually created gold set:\n",
    "# /content/BaySIDshot/manual_data/alllangs_eval_data # or just dialects_eval_data, etc.\n",
    "\n",
    "%cd /content/BaySIDshot/machamp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## mBERT:"
   ],
   "metadata": {
    "id": "O5CNokSkohDR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# inspect configs to be used\n",
    "! cat /content/BaySIDshot/configs/nlu_x.json\n",
    "\n",
    "# inspect params to be used\n",
    "! cat /content/BaySIDshot/configs/params_mbert.json"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ep39dLom-5Qu",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716021014681,
     "user_tz": -120,
     "elapsed": 538,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "outputId": "40b21b6d-5a94-4110-9300-b1ccb2f5a421",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train baseline for mBERT\n",
    "# make sure be in ../machamp or use direct path as below\n",
    "# set name of experiment / logs directory! --name Baseline_mBERT_SEED\n",
    "# with respective random seed that should be used --seed 1234 e.g.\n",
    "\n",
    "! python3 /content/BaySIDshot/machamp/train.py --dataset_configs /content/BaySIDshot/configs/nlu_x.json --parameters /content/BaySIDshot/configs/params_mbert.json --device 0 --name Baseline_mBERT_1234 --seed 1234\n"
   ],
   "metadata": {
    "id": "g-IQ2moIuvx_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f010ab04-0fbe-46c8-dde5-b706f484e822",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716027802477,
     "user_tz": -120,
     "elapsed": 2171576,
     "user": {
      "displayName": "Xaver Krueckl",
      "userId": "04994225532745810101"
     }
    },
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save logs dir with model, metrics and predictions to drive - change name of logs dir accordingly!\n",
    "\n",
    "! cp -R /content/BaySIDshot/machamp/logs/Baseline_mBERT_1234 /content/drive/MyDrive/Masterarbeit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SET and get path(s) to the final model\n",
    "# last line with text in model_path has model path to nlu model\n",
    "# (if nlu config was set as last one when sequential/intermediate training)\n",
    "! ls -d /content/drive/MyDrive/Masterarbeit/Baseline_mBERT_1234*/*/model_* > model_path.txt\n",
    "\n",
    "import os\n",
    "\n",
    "with open('model_path.txt', 'r') as file:\n",
    "  lines = file.readlines()\n",
    "  # get the path to the final model (maily if sequential experiment)\n",
    "  model_line = [line.strip() for line in lines if line.strip()][-1]\n",
    "  model_line = model_line.strip()\n",
    "  print(\"Evaluating Model: \", model_line)\n",
    "  if '/' in model_line:\n",
    "    # just to show the parts of the path:\n",
    "    parts = model_line.split('/')\n",
    "    # get all the necessary path parts:\n",
    "    model = model_line.split('/')[7]\n",
    "    time = model_line.split('/')[6]\n",
    "    experiment_name = model_line.split('/')[5]\n",
    "    if experiment_name.split(\".\"):\n",
    "      experiment_name_cleaned = experiment_name.split(\".\")[0]\n",
    "    save_dir = model_line.split('/')[:5]\n",
    "    base_save_dir = '/'.join(save_dir)\n",
    "  else:\n",
    "    raise ValueError(\"No valid model path\")\n",
    "\n",
    "\n",
    "#eval_dir = \"/content/BaySIDshot/manual_data/alllangs_eval_data\"\n",
    "eval_dir = \"/content/BaySIDshot/alllangs_eval_data\"\n",
    "# other evaluation sets can be used here\n",
    "\n",
    "\n",
    "with open('script.sh', 'w') as file:\n",
    "    # create predictions folder in directory where model is saved:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # prepare prediction files order\n",
    "    file_list = []\n",
    "    for filename in os.listdir(eval_dir):\n",
    "        if filename.endswith('test.conll'): # do not use when also using valid files is desired\n",
    "          # path to goldfile:\n",
    "          goldfile = eval_dir + \"/\" + filename\n",
    "          # path to outfile\n",
    "          outfile = base_save_dir + \"/\" + experiment_name + \"/\" + time + \"/predictions_\" + experiment_name_cleaned + \"/\" + filename + \".out\"\n",
    "          # append goldfile outfile \"pairs\" to filelist for prediction command\n",
    "          file_list.append(str(goldfile))\n",
    "          file_list.append(str(outfile))\n",
    "    file_list_string = ' '.join(file_list)\n",
    "    # prediction call for all files in test dir\n",
    "    # also adds the specific dataset for which prediction should be done - necessary in multitask setting, else possible to also append\n",
    "    file.write(f\"! python3 predict.py {model_line} {file_list_string} --device 0 --dataset NLU\\n\")\n",
    "    # dir eval call to get all metrics and save output as json file:\n",
    "    file.write(f\"! python3 /content/BaySIDshot/scripts/dir_eval_out.py {eval_dir} {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # copy json output also to predictions dir:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/results\\n\")\n",
    "    file.write(f\"! cp /content/BaySIDshot/results/* {base_save_dir}/results\\n\")\n",
    "\n",
    "print(\"Prediction Script successfully generated!\")\n",
    "\n",
    "# runs predictions:\n",
    "! bash script.sh\n",
    "# and removes scripts:\n",
    "! rm script.sh\n",
    "! rm model_path.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XLM-R:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inspect configs to be used\n",
    "! cat /content/BaySIDshot/configs/nlu_x.json\n",
    "\n",
    "# inspect params to be used\n",
    "! cat /content/BaySIDshot/configs/params_xlmr.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train baseline for XLMR\n",
    "# make sure be in ../machamp or use direct path as below\n",
    "# set name of experiment / logs directory! --name Baseline_XLMR_SEED\n",
    "# with respective random seed that should be used --seed 1234 e.g.\n",
    "\n",
    "! python3 /content/BaySIDshot/machamp/train.py --dataset_configs /content/BaySIDshot/configs/nlu_x.json --parameters /content/BaySIDshot/configs/params_xlmr.json --device 0 --name Baseline_XLMR_1234 --seed 1234\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save logs dir with model, metrics and predictions to drive - change name of logs dir accordingly!\n",
    "\n",
    "! cp -R /content/BaySIDshot/machamp/logs/Baseline_XLMR_1234 /content/drive/MyDrive/Masterarbeit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SET and get path(s) to the final model\n",
    "# last line with text in model_path has model path to nlu model\n",
    "# (if nlu config was set as last one when sequential/intermediate training)\n",
    "! ls -d /content/drive/MyDrive/Masterarbeit/Baseline_mBERT_1234*/*/model_* > model_path.txt\n",
    "\n",
    "import os\n",
    "\n",
    "with open('model_path.txt', 'r') as file:\n",
    "  lines = file.readlines()\n",
    "  # get the path to the final model (maily if sequential experiment)\n",
    "  model_line = [line.strip() for line in lines if line.strip()][-1]\n",
    "  model_line = model_line.strip()\n",
    "  print(\"Evaluating Model: \", model_line)\n",
    "  if '/' in model_line:\n",
    "    # just to show the parts of the path:\n",
    "    parts = model_line.split('/')\n",
    "    # get all the necessary path parts:\n",
    "    model = model_line.split('/')[7]\n",
    "    time = model_line.split('/')[6]\n",
    "    experiment_name = model_line.split('/')[5]\n",
    "    if experiment_name.split(\".\"):\n",
    "      experiment_name_cleaned = experiment_name.split(\".\")[0]\n",
    "    save_dir = model_line.split('/')[:5]\n",
    "    base_save_dir = '/'.join(save_dir)\n",
    "  else:\n",
    "    raise ValueError(\"No valid model path\")\n",
    "\n",
    "\n",
    "#eval_dir = \"/content/BaySIDshot/manual_data/alllangs_eval_data\"\n",
    "eval_dir = \"/content/BaySIDshot/alllangs_eval_data\"\n",
    "# other evaluation sets can be used here\n",
    "\n",
    "\n",
    "with open('script.sh', 'w') as file:\n",
    "    # create predictions folder in directory where model is saved:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # prepare prediction files order\n",
    "    file_list = []\n",
    "    for filename in os.listdir(eval_dir):\n",
    "        if filename.endswith('test.conll'): # do not use when also using valid files is desired\n",
    "          # path to goldfile:\n",
    "          goldfile = eval_dir + \"/\" + filename\n",
    "          # path to outfile\n",
    "          outfile = base_save_dir + \"/\" + experiment_name + \"/\" + time + \"/predictions_\" + experiment_name_cleaned + \"/\" + filename + \".out\"\n",
    "          # append goldfile outfile \"pairs\" to filelist for prediction command\n",
    "          file_list.append(str(goldfile))\n",
    "          file_list.append(str(outfile))\n",
    "    file_list_string = ' '.join(file_list)\n",
    "    # prediction call for all files in test dir\n",
    "    # also adds the specific dataset for which prediction should be done - necessary in multitask setting, else possible to also append\n",
    "    file.write(f\"! python3 predict.py {model_line} {file_list_string} --device 0 --dataset NLU\\n\")\n",
    "    # dir eval call to get all metrics and save output as json file:\n",
    "    file.write(f\"! python3 /content/BaySIDshot/scripts/dir_eval_out.py {eval_dir} {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # copy json output also to predictions dir:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/results\\n\")\n",
    "    file.write(f\"! cp /content/BaySIDshot/results/* {base_save_dir}/results\\n\")\n",
    "\n",
    "print(\"Prediction Script successfully generated!\")\n",
    "\n",
    "# runs predictions:\n",
    "! bash script.sh\n",
    "# and removes scripts:\n",
    "! rm script.sh\n",
    "! rm model_path.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## mDeBERTa:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inspect configs to be used\n",
    "! cat /content/BaySIDshot/configs/nlu_x.json\n",
    "\n",
    "# inspect params to be used\n",
    "! cat /content/BaySIDshot/configs/params_mdeberta.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train baseline for mDeBERTa\n",
    "# make sure be in ../machamp or use direct path as below\n",
    "# set name of experiment / logs directory! --name Baseline_mDeBERTa_SEED\n",
    "# with respective random seed that should be used --seed 1234 e.g.\n",
    "\n",
    "! python3 /content/BaySIDshot/machamp/train.py --dataset_configs /content/BaySIDshot/configs/nlu_x.json --parameters /content/BaySIDshot/configs/params_mdeberta.json --device 0 --name Baseline_mDeBERTa_1234 --seed 1234\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save logs dir with model, metrics and predictions to drive - change name of logs dir accordingly!\n",
    "\n",
    "! cp -R /content/BaySIDshot/machamp/logs/Baseline_mDeBERTa_1234 /content/drive/MyDrive/Masterarbeit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SET and get path(s) to the final model\n",
    "# last line with text in model_path has model path to nlu model\n",
    "# (if nlu config was set as last one when sequential/intermediate training)\n",
    "! ls -d /content/drive/MyDrive/Masterarbeit/Baseline_mDeBERTa_1234*/*/model_* > model_path.txt\n",
    "\n",
    "import os\n",
    "\n",
    "with open('model_path.txt', 'r') as file:\n",
    "  lines = file.readlines()\n",
    "  # get the path to the final model (maily if sequential experiment)\n",
    "  model_line = [line.strip() for line in lines if line.strip()][-1]\n",
    "  model_line = model_line.strip()\n",
    "  print(\"Evaluating Model: \", model_line)\n",
    "  if '/' in model_line:\n",
    "    # just to show the parts of the path:\n",
    "    parts = model_line.split('/')\n",
    "    # get all the necessary path parts:\n",
    "    model = model_line.split('/')[7]\n",
    "    time = model_line.split('/')[6]\n",
    "    experiment_name = model_line.split('/')[5]\n",
    "    if experiment_name.split(\".\"):\n",
    "      experiment_name_cleaned = experiment_name.split(\".\")[0]\n",
    "    save_dir = model_line.split('/')[:5]\n",
    "    base_save_dir = '/'.join(save_dir)\n",
    "  else:\n",
    "    raise ValueError(\"No valid model path\")\n",
    "\n",
    "\n",
    "#eval_dir = \"/content/BaySIDshot/manual_data/alllangs_eval_data\"\n",
    "eval_dir = \"/content/BaySIDshot/alllangs_eval_data\"\n",
    "# other evaluation sets can be used here\n",
    "\n",
    "\n",
    "with open('script.sh', 'w') as file:\n",
    "    # create predictions folder in directory where model is saved:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # prepare prediction files order\n",
    "    file_list = []\n",
    "    for filename in os.listdir(eval_dir):\n",
    "        if filename.endswith('test.conll'): # do not use when also using valid files is desired\n",
    "          # path to goldfile:\n",
    "          goldfile = eval_dir + \"/\" + filename\n",
    "          # path to outfile\n",
    "          outfile = base_save_dir + \"/\" + experiment_name + \"/\" + time + \"/predictions_\" + experiment_name_cleaned + \"/\" + filename + \".out\"\n",
    "          # append goldfile outfile \"pairs\" to filelist for prediction command\n",
    "          file_list.append(str(goldfile))\n",
    "          file_list.append(str(outfile))\n",
    "    file_list_string = ' '.join(file_list)\n",
    "    # prediction call for all files in test dir\n",
    "    # also adds the specific dataset for which prediction should be done - necessary in multitask setting, else possible to also append\n",
    "    file.write(f\"! python3 predict.py {model_line} {file_list_string} --device 0 --dataset NLU\\n\")\n",
    "    # dir eval call to get all metrics and save output as json file:\n",
    "    file.write(f\"! python3 /content/BaySIDshot/scripts/dir_eval_out.py {eval_dir} {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # copy json output also to predictions dir:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/results\\n\")\n",
    "    file.write(f\"! cp /content/BaySIDshot/results/* {base_save_dir}/results\\n\")\n",
    "\n",
    "print(\"Prediction Script successfully generated!\")\n",
    "\n",
    "# runs predictions:\n",
    "! bash script.sh\n",
    "# and removes scripts:\n",
    "! rm script.sh\n",
    "! rm model_path.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## gBERT:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inspect configs to be used\n",
    "! cat /content/BaySIDshot/configs/nlu_x.json\n",
    "\n",
    "# inspect params to be used\n",
    "! cat /content/BaySIDshot/configs/params_gbert.json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train baseline for gBERT\n",
    "# make sure be in ../machamp or use direct path as below\n",
    "# set name of experiment / logs directory! --name Baseline_gBERT_SEED\n",
    "# with respective random seed that should be used --seed 1234 e.g.\n",
    "\n",
    "! python3 /content/BaySIDshot/machamp/train.py --dataset_configs /content/BaySIDshot/configs/nlu_x.json --parameters /content/BaySIDshot/configs/params_gbert.json --device 0 --name Baseline_gBERT_1234 --seed 1234\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save logs dir with model, metrics and predictions to drive - change name of logs dir accordingly!\n",
    "\n",
    "! cp -R /content/BaySIDshot/machamp/logs/Baseline_gBERT_1234 /content/drive/MyDrive/Masterarbeit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SET and get path(s) to the final model\n",
    "# last line with text in model_path has model path to nlu model\n",
    "# (if nlu config was set as last one when sequential/intermediate training)\n",
    "! ls -d /content/drive/MyDrive/Masterarbeit/Baseline_gBERT_1234*/*/model_* > model_path.txt\n",
    "\n",
    "import os\n",
    "\n",
    "with open('model_path.txt', 'r') as file:\n",
    "  lines = file.readlines()\n",
    "  # get the path to the final model (maily if sequential experiment)\n",
    "  model_line = [line.strip() for line in lines if line.strip()][-1]\n",
    "  model_line = model_line.strip()\n",
    "  print(\"Evaluating Model: \", model_line)\n",
    "  if '/' in model_line:\n",
    "    # just to show the parts of the path:\n",
    "    parts = model_line.split('/')\n",
    "    # get all the necessary path parts:\n",
    "    model = model_line.split('/')[7]\n",
    "    time = model_line.split('/')[6]\n",
    "    experiment_name = model_line.split('/')[5]\n",
    "    if experiment_name.split(\".\"):\n",
    "      experiment_name_cleaned = experiment_name.split(\".\")[0]\n",
    "    save_dir = model_line.split('/')[:5]\n",
    "    base_save_dir = '/'.join(save_dir)\n",
    "  else:\n",
    "    raise ValueError(\"No valid model path\")\n",
    "\n",
    "\n",
    "#eval_dir = \"/content/BaySIDshot/manual_data/alllangs_eval_data\"\n",
    "eval_dir = \"/content/BaySIDshot/alllangs_eval_data\"\n",
    "# other evaluation sets can be used here\n",
    "\n",
    "\n",
    "with open('script.sh', 'w') as file:\n",
    "    # create predictions folder in directory where model is saved:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # prepare prediction files order\n",
    "    file_list = []\n",
    "    for filename in os.listdir(eval_dir):\n",
    "        if filename.endswith('test.conll'): # do not use when also using valid files is desired\n",
    "          # path to goldfile:\n",
    "          goldfile = eval_dir + \"/\" + filename\n",
    "          # path to outfile\n",
    "          outfile = base_save_dir + \"/\" + experiment_name + \"/\" + time + \"/predictions_\" + experiment_name_cleaned + \"/\" + filename + \".out\"\n",
    "          # append goldfile outfile \"pairs\" to filelist for prediction command\n",
    "          file_list.append(str(goldfile))\n",
    "          file_list.append(str(outfile))\n",
    "    file_list_string = ' '.join(file_list)\n",
    "    # prediction call for all files in test dir\n",
    "    # also adds the specific dataset for which prediction should be done - necessary in multitask setting, else possible to also append\n",
    "    file.write(f\"! python3 predict.py {model_line} {file_list_string} --device 0 --dataset NLU\\n\")\n",
    "    # dir eval call to get all metrics and save output as json file:\n",
    "    file.write(f\"! python3 /content/BaySIDshot/scripts/dir_eval_out.py {eval_dir} {base_save_dir}/{experiment_name}/{time}/predictions_{experiment_name_cleaned}\\n\")\n",
    "    # copy json output also to predictions dir:\n",
    "    file.write(f\"! mkdir -p {base_save_dir}/results\\n\")\n",
    "    file.write(f\"! cp /content/BaySIDshot/results/* {base_save_dir}/results\\n\")\n",
    "\n",
    "print(\"Prediction Script successfully generated!\")\n",
    "\n",
    "# runs predictions:\n",
    "! bash script.sh\n",
    "# and removes scripts:\n",
    "! rm script.sh\n",
    "! rm model_path.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhcUQ7_A1y0t"
   },
   "source": [
    "### Reference for _MaChAmp_:\n",
    "\n",
    "```\n",
    "@inproceedings{van-der-goot-etal-2021-massive,\n",
    "    title = \"Massive Choice, Ample Tasks ({M}a{C}h{A}mp): A Toolkit for Multi-task Learning in {NLP}\",\n",
    "    author = {van der Goot, Rob  and\n",
    "      {\\\"U}st{\\\"u}n, Ahmet  and\n",
    "      Ramponi, Alan  and\n",
    "      Sharaf, Ibrahim  and\n",
    "      Plank, Barbara},\n",
    "    booktitle = \"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations\",\n",
    "    month = apr,\n",
    "    year = \"2021\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/2021.eacl-demos.22\",\n",
    "    doi = \"10.18653/v1/2021.eacl-demos.22\",\n",
    "    pages = \"176--197\",\n",
    "    abstract = \"Transfer learning, particularly approaches that combine multi-task learning with pre-trained contextualized embeddings and fine-tuning, have advanced the field of Natural Language Processing tremendously in recent years. In this paper we present MaChAmp, a toolkit for easy fine-tuning of contextualized embeddings in multi-task settings. The benefits of MaChAmp are its flexible configuration options, and the support of a variety of natural language processing tasks in a uniform toolkit, from text classification and sequence labeling to dependency parsing, masked language modeling, and text generation.\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "PZkTDjBxD6Fs"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "V100",
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
